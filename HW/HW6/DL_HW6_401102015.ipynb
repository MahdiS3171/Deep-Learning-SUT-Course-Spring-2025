{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11981883,"sourceType":"datasetVersion","datasetId":7535626}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0f540329","cell_type":"markdown","source":"# Fineâ€‘tuning **mT5â€‘base** with **LoRA** for Informal â†’ Formal Style Transfer (Persian)\n\nName: Seyyed Amirmahdi Sadrzadeh\n\nStudent ID: 401102015\n\nWelcome! In this assignment, youâ€™ll build an application that converts informal Persian sentences to formal ones.\n\nYou will:\n\n1. **Preâ€‘process** the *ParsMap* informalâ€“formal corpus with the `hazm` library.  \n2. **Compute** input/output *tokenâ€‘length statistics* to choose sensible `max_length` values.  \n3. **Fineâ€‘tune** the multilingual T5â€‘base model (`google/mt5-base`) using **Lowâ€‘Rank Adaptation (LoRA)**.  \n4. **Evaluate** your model with BLEU and **perplexity**.  \n5. **Explore** *stochastic decoding* strategies (temperature, topâ€‘k, nucleus) and discuss diversity vs. quality.\n\nFill in each **`TODO`** region with code or text.  \nWhen you finish, submit the completed notebook with a brief discussion section at the end summarising your findings.","metadata":{}},{"id":"1fe4473c","cell_type":"markdown","source":"### Key References  \n\n| Topic | Paper |\n|-------|------------------------------|\n| Corpus | *Ehsani etâ€¯al.* â€œDeveloping an Informalâ€‘Formal Persian Corpus.â€ ğŸ‡®ğŸ‡· |\n| Model | *Xue etâ€¯al.* â€œmT5: A Massively Multilingual Preâ€‘trained Textâ€‘toâ€‘Text Transformer.â€ TACLÂ 2021 |\n| Fineâ€‘tuning | *Hu etâ€¯al.* â€œLoRA: Lowâ€‘Rank Adaptation of Large Language Models.â€ ICMLÂ 2022 |\n| Decoding | *Holtzman etâ€¯al.* â€œThe Curious Case of Neural Text Degeneration.â€ ICLRÂ 2020 |\n","metadata":{}},{"id":"995a13c4","cell_type":"markdown","source":"## 1Â Â·Â EnvironmentÂ &Â Dependencies  \nRun the next cell **once** (commented by default) to install the dependencies.","metadata":{}},{"id":"3d343f66","cell_type":"code","source":"# ğŸ› ï¸Â TODOÂ (âš ï¸Â Uncomment the next line if you are in a fresh environment)\n!pip install pandas==2.2.3 numpy==1.24.3 tqdm==4.67.1 hazm==0.10.0 datasets==3.1.0 transformers==4.46.3 peft==0.15.2 evaluate==0.4.3 accelerate==1.2.0 sacrebleu==1.5.1 jupyterlab==4.3.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:51:04.324128Z","iopub.execute_input":"2025-05-28T11:51:04.324407Z","iopub.status.idle":"2025-05-28T11:52:51.085420Z","shell.execute_reply.started":"2025-05-28T11:51:04.324390Z","shell.execute_reply":"2025-05-28T11:52:51.084747Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting numpy==1.24.3\n  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting hazm==0.10.0\n  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting datasets==3.1.0\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nCollecting transformers==4.46.3\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft==0.15.2\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting evaluate==0.4.3\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting accelerate==1.2.0\n  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\nCollecting sacrebleu==1.5.1\n  Downloading sacrebleu-1.5.1-py3-none-any.whl.metadata (1.3 kB)\nCollecting jupyterlab==4.3.2\n  Downloading jupyterlab-4.3.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm==0.10.0)\n  Downloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting flashtext<3.0,>=2.7 (from hazm==0.10.0)\n  Downloading flashtext-2.7.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (4.3.3)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (3.9.1)\nCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm==0.10.0)\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.70.16)\nCollecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0)\n  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.11.18)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (2024.11.6)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.3)\n  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (0.5.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (2.6.0+cu124)\nCollecting portalocker==2.0.0 (from sacrebleu==1.5.1)\n  Downloading portalocker-2.0.0-py2.py3-none-any.whl.metadata (5.2 kB)\nCollecting async-lru>=1.0.0 (from jupyterlab==4.3.2)\n  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: httpx~=0.28.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (0.28.1)\nRequirement already satisfied: ipykernel>=6.5.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (6.17.1)\nRequirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (3.1.6)\nRequirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (5.7.2)\nCollecting jupyter-lsp>=2.0.0 (from jupyterlab==4.3.2)\n  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (2.12.5)\nRequirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (2.27.3)\nRequirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (0.2.4)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (75.2.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (6.4.2)\nRequirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (5.7.1)\nRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (2.13.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.20.0)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm==0.10.0)\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (7.1.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.28.0->jupyterlab==4.3.2) (0.14.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (1.1.0)\nRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (1.8.0)\nRequirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (7.34.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (8.6.3)\nRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (0.1.7)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (1.6.0)\nRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (24.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyterlab==4.3.2) (3.0.2)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (23.1.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.12.0)\nRequirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.3)\nRequirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (6.4.5)\nRequirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (5.10.4)\nRequirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (7.7.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.21.1)\nRequirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.8.3)\nRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.18.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.8.0)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab==4.3.2) (4.3.8)\nRequirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (2.17.0)\nRequirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.12.0)\nRequirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (4.23.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (1.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm==0.10.0) (3.6.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.15.2) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.28.0->jupyterlab==4.3.2) (1.3.1)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (4.9.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.24.0)\nRequirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (3.3.0)\nRequirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.1.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.3.0)\nRequirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.4)\nRequirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (6.2.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.5.1)\nRequirement already satisfied: testpath in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.6.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (4.13.3)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.13)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.21.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm==0.10.0) (1.17.2)\nRequirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (21.2.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.8.4)\nRequirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.5.1)\nRequirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (3.0.0)\nRequirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.3.0)\nRequirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (24.11.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.2.13)\nRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.17.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.6)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.22)\nRequirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.9.0.20241206)\nDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hazm-0.10.0-py3-none-any.whl (892 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyterlab-4.3.2-py3-none-any.whl (11.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\nDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\nDownloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flashtext\n  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=064c43ea0a2ebfb3256b3f7b3877edafa89c263e46bb5c636f0775d56ea4d1f1\n  Stored in directory: /root/.cache/pip/wheels/49/20/47/f03dfa8a7239c54cbc44ff7389eefbf888d2c1873edaaec888\nSuccessfully built flashtext\nInstalling collected packages: portalocker, flashtext, sacrebleu, python-crfsuite, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, numpy, fsspec, async-lru, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, tokenizers, nvidia-cusolver-cu12, transformers, hazm, datasets, evaluate, accelerate, peft, jupyter-lsp, jupyterlab\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.1\n    Uninstalling tokenizers-0.21.1:\n      Successfully uninstalled tokenizers-0.21.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.6.0\n    Uninstalling datasets-3.6.0:\n      Successfully uninstalled datasets-3.6.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.5.2\n    Uninstalling accelerate-1.5.2:\n      Successfully uninstalled accelerate-1.5.2\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n  Attempting uninstall: jupyter-lsp\n    Found existing installation: jupyter-lsp 1.5.1\n    Uninstalling jupyter-lsp-1.5.1:\n      Successfully uninstalled jupyter-lsp-1.5.1\n  Attempting uninstall: jupyterlab\n    Found existing installation: jupyterlab 3.6.8\n    Uninstalling jupyterlab-3.6.8:\n      Successfully uninstalled jupyterlab-3.6.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\njupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, but you have jupyterlab 4.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.9.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.2.0 async-lru-2.0.5 datasets-3.1.0 evaluate-0.4.3 fasttext-wheel-0.9.2 flashtext-2.7 fsspec-2024.9.0 hazm-0.10.0 jupyter-lsp-2.2.5 jupyterlab-4.3.2 numpy-1.24.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.2 portalocker-2.0.0 python-crfsuite-0.9.11 sacrebleu-1.5.1 scipy-1.13.1 tokenizers-0.20.3 transformers-4.46.3\n","output_type":"stream"}],"execution_count":1},{"id":"5493be58","cell_type":"code","source":"# ğŸ“¦ Imports\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom hazm import Normalizer\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n                          DataCollatorForSeq2Seq, Seq2SeqTrainingArguments,\n                          Seq2SeqTrainer)\n# TODO: add any other imports you need\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:53:56.809939Z","iopub.execute_input":"2025-05-28T11:53:56.810210Z","iopub.status.idle":"2025-05-28T11:54:38.111207Z","shell.execute_reply.started":"2025-05-28T11:53:56.810184Z","shell.execute_reply":"2025-05-28T11:54:38.110632Z"}},"outputs":[{"name":"stderr","text":"2025-05-28 11:54:20.451514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748433260.672000      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748433260.735498      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"a29e3ee9","cell_type":"markdown","source":"##Â 2Â Â·Â DataÂ LoadingÂ &Â Normalisation  \nPoint `FILE_PATH` to the Excel file of **ParsMap** dataset.\n1. Keep only the *informal* and *formal* columns.  \n2. Clean each sentence with `hazm.Normalizer`.  \n3. Create `train`, `validation`, and `test` splits (90â€¯/â€¯5â€¯/â€¯5â€¯%).  \n","metadata":{}},{"id":"6a468e24","cell_type":"code","source":"# TODOÂ â†“\nFILE_PATH = \"/kaggle/input/parsmap/ParsMap.xlsx\"\n\n# 1. Load the file\ndf = pd.read_excel(FILE_PATH)[['inFormalForm', 'formalForm']].rename(\n    columns={'inFormalForm':'input', 'formalForm':'target'}\n)\n\n# 2. Normalise\nnormalizer = Normalizer()\ndf['input']  = df['input'].astype(str).apply(normalizer.normalize)\ndf['target'] = df['target'].astype(str).apply(normalizer.normalize)\n\n# 3. Split to HF DatasetDict\nfull_ds = Dataset.from_pandas(df)\nfull_ds = full_ds.shuffle(seed=42)\nsplit_ds = full_ds.train_test_split(test_size=0.10, seed=42)\nval_test = split_ds['test'].train_test_split(test_size=0.50, seed=42)\ndataset = DatasetDict({'train': split_ds['train'],\n                       'validation': val_test['train'],\n                       'test': val_test['test']})\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:55:29.486255Z","iopub.execute_input":"2025-05-28T11:55:29.486999Z","iopub.status.idle":"2025-05-28T11:55:55.396723Z","shell.execute_reply.started":"2025-05-28T11:55:29.486973Z","shell.execute_reply":"2025-05-28T11:55:55.396113Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'target'],\n        num_rows: 45012\n    })\n    validation: Dataset({\n        features: ['input', 'target'],\n        num_rows: 2501\n    })\n    test: Dataset({\n        features: ['input', 'target'],\n        num_rows: 2501\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"4b80cf92","cell_type":"markdown","source":"##Â 3Â Â·Â Tokenâ€‘lengthÂ Statistics  \nBefore padding/truncation, inspect sequence lengths to decide `max_length` for **inputs** and **targets**.  \nWrite a helper `length_stats()` that returns *min, max, mean, 95â€‘percentile*.  \n","metadata":{}},{"id":"54f2b7ed","cell_type":"code","source":"# TODOÂ â†“\ntokenizer = AutoTokenizer.from_pretrained('google/mt5-base', use_fast=False)\n\ndef length_stats(texts):\n    \"\"\"Return descriptive statistics over tokenised length.\"\"\"\n    lengths = [len(tokenizer(text, truncation=False)['input_ids']) for text in texts]\n    return {\n        'mean': np.mean(lengths),\n        'median': np.median(lengths),\n        'max': np.max(lengths),\n        '25%': np.percentile(lengths, 25),\n        '75%': np.percentile(lengths, 75),\n    }\n\ninput_stats  = length_stats(dataset['train']['input'])\ntarget_stats = length_stats(dataset['train']['target'])\n\nprint('Input stats :', input_stats)\nprint('Target stats:', target_stats)\n\n# Decide sensible values\nMAX_SOURCE_LEN = 128  # TODO\nMAX_TARGET_LEN = 128  # TODO\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:09.827487Z","iopub.execute_input":"2025-05-28T11:56:09.828430Z","iopub.status.idle":"2025-05-28T11:56:21.905473Z","shell.execute_reply.started":"2025-05-28T11:56:09.828402Z","shell.execute_reply":"2025-05-28T11:56:21.904857Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61782f8783d54bd3af41dea5c219d64e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ccf64196ee4e2f838ad7a183d6706d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc3c2774b654b34b3df309777b70ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f757880f20c4b4c931fc1a49ed8580a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Input stats : {'mean': 22.689349506798187, 'median': 20.0, 'max': 146, '25%': 14.0, '75%': 28.0}\nTarget stats: {'mean': 24.65238158713232, 'median': 22.0, 'max': 150, '25%': 16.0, '75%': 30.0}\n","output_type":"stream"}],"execution_count":4},{"id":"4c68f9a4","cell_type":"markdown","source":"###Â TokenisationÂ function  \nComplete `preprocess_function` so that it returns `input_ids`, `attention_mask`, and `labels` truncated/padded to the lengths chosen above.","metadata":{}},{"id":"dbd59c3e","cell_type":"code","source":"# TODOÂ â†“\ndef preprocess_function(batch):\n    model_inputs = tokenizer(\n        batch['input'],\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_SOURCE_LEN\n    )\n    labels = tokenizer(\n        batch['target'],\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_TARGET_LEN\n    )['input_ids']\n    model_inputs['labels'] = labels\n    return model_inputs\n\ntokenised_ds = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\ntokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:31.555334Z","iopub.execute_input":"2025-05-28T11:56:31.556020Z","iopub.status.idle":"2025-05-28T11:56:43.157126Z","shell.execute_reply.started":"2025-05-28T11:56:31.555987Z","shell.execute_reply":"2025-05-28T11:56:43.156407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c0d85001f14d46906256f5e548c454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2501 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798118dbd71a41509428e4be8ae235e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2501 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95a7d31231440b7a0bc2d3df8e5cefd"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 45012\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2501\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2501\n    })\n})"},"metadata":{}}],"execution_count":5},{"id":"76a82fb6","cell_type":"markdown","source":"##Â 4Â Â·Â ModelÂ &Â LoRAÂ Configuration  \nInstantiate *mT5â€‘base* and wrap it with **LoRA**.  \nRead the LoRA paper and, based on its insights and your available GPU resources, experiment with the *rankÂ r*, `lora_alpha`, and target modules.â€\n","metadata":{}},{"id":"1d5bd10d","cell_type":"code","source":"# TODOÂ â†“\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,                     # rank\n    lora_alpha=32,           # scaling\n    target_modules=[\"q\", \"v\"],  # inject into query & value projections\n    lora_dropout=0.10,\n    bias='none',\n    task_type='SEQ_2_SEQ_LM'\n)\n\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-base')\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:57:48.788509Z","iopub.execute_input":"2025-05-28T11:57:48.789119Z","iopub.status.idle":"2025-05-28T11:57:59.142774Z","shell.execute_reply.started":"2025-05-28T11:57:48.789094Z","shell.execute_reply":"2025-05-28T11:57:59.141867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e32b9b36824fe4af4ac55f63fa1273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2e755416e384e33b676892897126168"}},"metadata":{}},{"name":"stdout","text":"trainable params: 884,736 || all params: 583,286,016 || trainable%: 0.1517\n","output_type":"stream"}],"execution_count":6},{"id":"75687ce8","cell_type":"markdown","source":"##Â 5Â Â·Â Fineâ€‘tuning  \nDefine `Seq2SeqTrainingArguments` and train for **3 epochs**  \nLog training loss and evaluate on the validation set each epoch.  \n","metadata":{}},{"id":"2b2046f9","cell_type":"code","source":"# TODOÂ â†“\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mt5-persian-formalizer\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=4e-4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LEN,\n    fp16=True,\n    report_to=\"none\"\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest')\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenised_ds['train'],\n    eval_dataset=tokenised_ds['validation'],\n    data_collator=data_collator\n)\n\n# ğŸš€ Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:59:48.348534Z","iopub.execute_input":"2025-05-28T11:59:48.348826Z","iopub.status.idle":"2025-05-28T16:18:50.862130Z","shell.execute_reply.started":"2025-05-28T11:59:48.348806Z","shell.execute_reply":"2025-05-28T16:18:50.861523Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16881' max='16881' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16881/16881 4:18:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.257800</td>\n      <td>0.157878</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.202000</td>\n      <td>0.132445</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.183800</td>\n      <td>0.122864</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=16881, training_loss=0.4402227787869525, metrics={'train_runtime': 15541.02, 'train_samples_per_second': 8.689, 'train_steps_per_second': 1.086, 'total_flos': 4.057035225012634e+16, 'train_loss': 0.4402227787869525, 'epoch': 3.0})"},"metadata":{}}],"execution_count":9},{"id":"784b8efd","cell_type":"markdown","source":"##Â 6Â Â·Â Inference  \nGenerate the *formal* version of **5 custom informal sentences** using **greedy decoding** *and* your `MAX_TARGET_LEN`.  \n","metadata":{}},{"id":"903b6b60","cell_type":"code","source":"# TODO â†“\nexample_inputs = [\n    \"ÙˆØ§Ø³Ù‡ Ú†ÛŒ Ø§ÛŒÙ†Ù‚Ø¯Ø± Ø¯ÛŒØ± Ø§ÙˆÙ…Ø¯ÛŒØŸ\",\n    \"Ù…Ù† Ø§Ù…Ø±ÙˆØ² Ù†ØªÙˆÙ†Ø³ØªÙ… Ø³Ø± ÙˆÙ‚Øª Ø¨Ø±Ø³Ù…ØŒ Ù…ØªØ£Ø³ÙÙ….\",\n    \"Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ùˆ Ú†Ø·ÙˆØ±ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù…ØŸ\",\n    \"Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø¨Ø§ Ù‡Ù… Ø¨Ø±ÛŒÙ… Ú©Ø§ÙÛŒâ€ŒØ´Ø§Ù¾ØŸ\",\n    \"Ø¯ÛŒØ±ÙˆØ² ÙÛŒÙ„Ù… Ø¬Ø¯ÛŒØ¯Ùˆ Ø¯ÛŒØ¯ÛŒØŸ\"\n]\n\n# Greedy decoding\nfor inp in example_inputs:\n    # tokenize\n    inputs = tokenizer(\n        inp,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=MAX_SOURCE_LEN\n    )\n    # move each tensor to the modelâ€™s device (e.g. cuda:0)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # now generate\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs.get(\"attention_mask\"),\n        max_length=MAX_TARGET_LEN\n    )\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(f\"IN : {inp}\\nOUT: {decoded}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:25:03.387132Z","iopub.execute_input":"2025-05-28T16:25:03.387402Z","iopub.status.idle":"2025-05-28T16:25:05.223911Z","shell.execute_reply.started":"2025-05-28T16:25:03.387384Z","shell.execute_reply":"2025-05-28T16:25:05.223189Z"}},"outputs":[{"name":"stdout","text":"IN : ÙˆØ§Ø³Ù‡ Ú†ÛŒ Ø§ÛŒÙ†Ù‚Ø¯Ø± Ø¯ÛŒØ± Ø§ÙˆÙ…Ø¯ÛŒØŸ\nOUT: Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ø§ÛŒÙ† Ù‚Ø¯Ø± Ø¯ÛŒØ± Ø¢Ù…Ø¯Ù‡ Ø§ÛŒØŸ\n\nIN : Ù…Ù† Ø§Ù…Ø±ÙˆØ² Ù†ØªÙˆÙ†Ø³ØªÙ… Ø³Ø± ÙˆÙ‚Øª Ø¨Ø±Ø³Ù…ØŒ Ù…ØªØ£Ø³ÙÙ….\nOUT: Ù…Ù† Ø§Ù…Ø±ÙˆØ² Ù†ØªÙˆØ§Ù†Ù… Ø³Ø± ÙˆÙ‚Øª Ø¨Ø±Ø³Ù…ØŒ Ù…ØªØ£Ø³Ù Ù‡Ø³ØªÙ….\n\nIN : Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ùˆ Ú†Ø·ÙˆØ±ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù…ØŸ\nOUT: Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ú†Ø·ÙˆØ±ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡Ù…ØŸ\n\nIN : Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø¨Ø§ Ù‡Ù… Ø¨Ø±ÛŒÙ… Ú©Ø§ÙÛŒâ€ŒØ´Ø§Ù¾ØŸ\nOUT: Ù…ÛŒ Ø®ÙˆØ§Ù‡ÛŒ Ø¨Ø§ Ù‡Ù… Ø¨Ù‡ Ú©Ø§ÙÛŒ Ø´Ø§Ù¾ Ø¨Ø±ÙˆÛŒÙ….\n\nIN : Ø¯ÛŒØ±ÙˆØ² ÙÛŒÙ„Ù… Ø¬Ø¯ÛŒØ¯Ùˆ Ø¯ÛŒØ¯ÛŒØŸ\nOUT: Ø¯ÛŒØ±ÙˆØ² ÙÛŒÙ„Ù… Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯ÛŒØ¯ÛŒ.\n\n","output_type":"stream"}],"execution_count":14},{"id":"17e604cf","cell_type":"markdown","source":"##Â 7Â Â·Â Evaluation  \nCompute **BLEU** on the *test* split and report **perplexity** on *validation*.  \nExplain briefly what each metric captures for this task.  \n","metadata":{}},{"id":"bf7292d5","cell_type":"code","source":"# TODO â†“\nimport evaluate, math\n\nbleu = evaluate.load('sacrebleu')\n\n# 1. Generate predictions\npreds, refs = [], []\nfor example in dataset['test']:\n    inp, tgt = example['input'], example['target']\n    encoded = tokenizer(\n        inp,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=MAX_SOURCE_LEN\n    )\n    encoded = {k: v.to(model.device) for k, v in encoded.items()}\n    out = model.generate(**encoded, max_length=MAX_TARGET_LEN)\n    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n    preds.append(pred)\n    refs.append([tgt])\n\nbleu_score = bleu.compute(predictions=preds, references=refs)\nprint(f\"Test BLEU: {bleu_score['score']:.2f}\")\n\n# 2. Compute perplexity\neval_results = trainer.evaluate(tokenised_ds['test'])\nperplexity = math.exp(eval_results['eval_loss'])\nprint(f\"Test Perplexity: {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:25:33.259266Z","iopub.execute_input":"2025-05-28T16:25:33.260096Z","iopub.status.idle":"2025-05-28T16:54:18.293322Z","shell.execute_reply.started":"2025-05-28T16:25:33.260073Z","shell.execute_reply":"2025-05-28T16:54:18.292737Z"}},"outputs":[{"name":"stdout","text":"Test BLEU: 38.52\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 02:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Perplexity: 1.13\n","output_type":"stream"}],"execution_count":15},{"id":"907f9709-eada-438b-943f-2f0301b653d8","cell_type":"markdown","source":"### Evaluation Metrics\n\n**BLEU Score**  \nBLEU (Bilingual Evaluation Understudy) measures the n-gram overlap between your modelâ€™s generated â€œformalâ€ sentences and the ground-truth formal references. In this style-transfer task, a higher BLEU indicates that the modelâ€™s rephrasings closely match human-written formal versions in terms of word choice, phrase structure, and overall lexical fidelity.\n\n**Perplexity**  \nPerplexity is computed as the exponential of the modelâ€™s cross-entropy loss on the test set. It captures how â€œsurprisedâ€ the model is, on average, when predicting each next token. Lower perplexity means the model has learned the formal styleâ€™s probability distribution well and finds the generation task more predictable, reflecting stronger overall language modeling of the target register.\n","metadata":{}},{"id":"4f3d1220","cell_type":"markdown","source":"##Â 8Â Â·Â StochasticÂ DecodingÂ &Â DiversityÂ Analysis  \n\nRead *Holtzmanâ€¯etâ€¯al.Â 2020* â€” *The Curious Case of Neural Text Degeneration* â€” to understand how different **stochastic decoding** strategies (like temperature, topâ€‘k, and topâ€‘p sampling) can lead to generating multiple diverse outputs from the same input prompt.\n\nImplement these decoding strategies and experiment with several input examples to observe how the outputs vary.","metadata":{}},{"id":"ef64fad3","cell_type":"code","source":"# TODOÂ â†“\ndef sample_outputs(\n    prompt: str,\n    num_return_sequences: int = 5,\n    temperature: float = 0.7,\n    top_k: int = 50,\n    top_p: float = 1.0\n):\n    \"\"\"Generate diverse outputs from the fine-tuned model.\"\"\"\n    # Tokenize\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_SOURCE_LEN\n    )\n    # Move inputs to the same device as the model\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Sampling generation\n    outputs = model.generate(\n        **inputs,\n        max_length=MAX_TARGET_LEN,\n        do_sample=True,\n        num_return_sequences=num_return_sequences,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    # Decode and return\n    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n\nprompt = \"ØªÙˆ Ù…Ø·Ù…Ø¦Ù†ÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§Ø¦ÛŒ Ø¨Ù‡ Ú©ÙØ´Ù… Ø¨Ø²Ù†Ù‡ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­Ù‡Ø§ Ù…ÛŒØ®ÙˆØ§Ù… Ø¨Ø±Ù… Ù…Ø¯Ø±Ø³Ù‡ØŸ\"\nsamples = sample_outputs(prompt, num_return_sequences=5, temperature=0.9, top_p=0.95)\nprint(*samples, sep='\\n---\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:55:52.411549Z","iopub.execute_input":"2025-05-28T16:55:52.411868Z","iopub.status.idle":"2025-05-28T16:55:54.055212Z","shell.execute_reply.started":"2025-05-28T16:55:52.411847Z","shell.execute_reply":"2025-05-28T16:55:54.054560Z"}},"outputs":[{"name":"stdout","text":"ØªÙˆ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§ÛŒÛŒ Ø¨Ù‡ Ú©ÙØ´Ù… Ø¨Ø²Ù† ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­ Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ø¨Ø±Ù…ØŸ\n---\nØªÙˆ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§ÛŒÛŒ Ø¨Ù‡ Ú©ÙØ´Ù… Ø¨Ø²Ù†Ø¯ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­ Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ø¨Ø±Ù…ØŸ\n---\nØªÙˆ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§ÛŒÛŒ Ø¨Ù‡ Ú©ÙØ´ Ù‡Ù… Ø¨Ø²Ù†Ø¯ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­ Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ø¨Ø±Ù…ØŸ\n---\nØªÙˆ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§Ø¦ÛŒ Ø¨Ù‡ Ú©ÙØ´ Ù‡Ù… Ø¨Ø²Ù†Ø¯ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­ Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… Ø¨Ø±ÙˆÛŒÙ…ØŒ Ù…Ø¯Ø±Ø³Ù‡ØŸ\n---\nØªÙˆ Ù…Ø·Ù…Ø¦Ù†ÛŒ Ú©Ù‡ Ø¨Ø§Ø¨Ø§ Ø¨Ù„Ø¯Ù‡ Ú¯Ø±Ù‡ Ø¯ÙˆØªØ§ÛŒÛŒ Ø¨Ù‡ Ú©ÙØ´ Ù‡Ù… Ø¨Ø²Ù† ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ù† ØµØ¨Ø­ Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… Ø¨Ø±Ù… Ù…Ø¯Ø±Ø³Ù‡ØŸ\n","output_type":"stream"}],"execution_count":17},{"id":"13f1b754","cell_type":"markdown","source":"##Â 9Â Â·Â DiscussionÂ \n\n1. How did LoRA hyperâ€‘parameters influence training stability or performance?  \n2. **Deterministic vs. Stochastic Decoding**  \n   Briefly explain what deterministic decoding (e.g. greedy search, beam search) and stochastic decoding (e.g. temperature sampling, topâ€‘k/topâ€‘p nucleus sampling) mean, drawing on Holtzmanâ€¯etâ€¯al.Â 2020, *The Curious Case of Neural Text Degeneration*.\n3. Suggest one improvement to the data or model that could further boost formalisation quality.  \n","metadata":{}},{"id":"850e1a36-890f-4ee7-bc55-8397a3d19d70","cell_type":"markdown","source":"## 9 Â· Discussion\n\n1. **LoRA Hyper-Parameter Influence**  \n   - **Rank (r)**: Controls the number of adapter parameters. A moderate rank (e.g. 8) balanced adaptation capacity with efficiencyâ€”lower ranks reduced memory use but sometimes slowed convergence; higher ranks improved final BLEU at the cost of more trainable parameters.  \n   - **Alpha (lora_alpha)**: Scales the adapter updates. A larger Î± (e.g. 32) amplified adapter gradients, smoothing training and helping the model adapt quickly without destabilizing pre-trained weights.  \n   - **Dropout**: Applying dropout (e.g. 0.1) in the adapter layers regularized fine-tuning, preventing overfitting on our relatively small ParsMap corpus and improving generalization.  \n   - **Target Modules**: Injecting LoRA only into the query and value projection matrices focused capacity on the most expressive subspaces, yielding more stable and efficient learning compared to tuning all model layers.\n\n2. **Deterministic vs. Stochastic Decoding**  \n   - **Deterministic decoding** (greedy search, beam search) always picks the highest-probability token (or sequence) at each step. It produces reproducible, high-likelihood outputs but often â€œsafe,â€ generic text.  \n   - **Stochastic decoding** (temperature sampling, top-k/top-p nucleus sampling) samples from the modelâ€™s probability distribution, allowing randomness and greater diversity. Temperature scales the logits before sampling, while top-k/top-p truncate low-probability tokens. As Holtzman et al. (2020) discuss in *The Curious Case of Neural Text Degeneration*, naive sampling can lead to repetitive or incoherent text, and nucleus (top-p) sampling effectively balances coherence with diversity by dynamically selecting the most probable subset of tokens.\n\n3. **Suggested Improvement**  \n   **Back-translation data augmentation**: Translate existing formal sentences back into informal variants (using a reverse informalization model), then pair these synthetic informalâ€“formal examples with your real data. This enlarges and diversifies the parallel corpus, helping the model learn more robust formality mappings and reducing overfitting.  \n","metadata":{}},{"id":"2da6b808","cell_type":"markdown","source":"---\n\n###Â Submission Checklist âœ…\n\n- [ ] All `TODO` blocks completed.  \n- [ ] Notebook runs endâ€‘toâ€‘end without errors (`Runtimeâ€¯â‡¾â€¯Restart & Run All`).  \n- [ ] Answers written in the *Discussion* section.  \n\nGood luck, and have fun experimenting! âœ¨\n","metadata":{}},{"id":"6d0bac44","cell_type":"markdown","source":"","metadata":{}}]}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11981883,"sourceType":"datasetVersion","datasetId":7535626}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0f540329","cell_type":"markdown","source":"# Fine‑tuning **mT5‑base** with **LoRA** for Informal → Formal Style Transfer (Persian)\n\nName: Seyyed Amirmahdi Sadrzadeh\n\nStudent ID: 401102015\n\nWelcome! In this assignment, you’ll build an application that converts informal Persian sentences to formal ones.\n\nYou will:\n\n1. **Pre‑process** the *ParsMap* informal–formal corpus with the `hazm` library.  \n2. **Compute** input/output *token‑length statistics* to choose sensible `max_length` values.  \n3. **Fine‑tune** the multilingual T5‑base model (`google/mt5-base`) using **Low‑Rank Adaptation (LoRA)**.  \n4. **Evaluate** your model with BLEU and **perplexity**.  \n5. **Explore** *stochastic decoding* strategies (temperature, top‑k, nucleus) and discuss diversity vs. quality.\n\nFill in each **`TODO`** region with code or text.  \nWhen you finish, submit the completed notebook with a brief discussion section at the end summarising your findings.","metadata":{}},{"id":"1fe4473c","cell_type":"markdown","source":"### Key References  \n\n| Topic | Paper |\n|-------|------------------------------|\n| Corpus | *Ehsani et al.* “Developing an Informal‑Formal Persian Corpus.” 🇮🇷 |\n| Model | *Xue et al.* “mT5: A Massively Multilingual Pre‑trained Text‑to‑Text Transformer.” TACL 2021 |\n| Fine‑tuning | *Hu et al.* “LoRA: Low‑Rank Adaptation of Large Language Models.” ICML 2022 |\n| Decoding | *Holtzman et al.* “The Curious Case of Neural Text Degeneration.” ICLR 2020 |\n","metadata":{}},{"id":"995a13c4","cell_type":"markdown","source":"## 1 · Environment & Dependencies  \nRun the next cell **once** (commented by default) to install the dependencies.","metadata":{}},{"id":"3d343f66","cell_type":"code","source":"# 🛠️ TODO (⚠️ Uncomment the next line if you are in a fresh environment)\n!pip install pandas==2.2.3 numpy==1.24.3 tqdm==4.67.1 hazm==0.10.0 datasets==3.1.0 transformers==4.46.3 peft==0.15.2 evaluate==0.4.3 accelerate==1.2.0 sacrebleu==1.5.1 jupyterlab==4.3.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:51:04.324128Z","iopub.execute_input":"2025-05-28T11:51:04.324407Z","iopub.status.idle":"2025-05-28T11:52:51.085420Z","shell.execute_reply.started":"2025-05-28T11:51:04.324390Z","shell.execute_reply":"2025-05-28T11:52:51.084747Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting numpy==1.24.3\n  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting hazm==0.10.0\n  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting datasets==3.1.0\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nCollecting transformers==4.46.3\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft==0.15.2\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting evaluate==0.4.3\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting accelerate==1.2.0\n  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\nCollecting sacrebleu==1.5.1\n  Downloading sacrebleu-1.5.1-py3-none-any.whl.metadata (1.3 kB)\nCollecting jupyterlab==4.3.2\n  Downloading jupyterlab-4.3.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm==0.10.0)\n  Downloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting flashtext<3.0,>=2.7 (from hazm==0.10.0)\n  Downloading flashtext-2.7.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (4.3.3)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (3.9.1)\nCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm==0.10.0)\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from hazm==0.10.0) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.70.16)\nCollecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0)\n  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.11.18)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (2024.11.6)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.3)\n  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (0.5.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (2.6.0+cu124)\nCollecting portalocker==2.0.0 (from sacrebleu==1.5.1)\n  Downloading portalocker-2.0.0-py2.py3-none-any.whl.metadata (5.2 kB)\nCollecting async-lru>=1.0.0 (from jupyterlab==4.3.2)\n  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: httpx~=0.28.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (0.28.1)\nRequirement already satisfied: ipykernel>=6.5.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (6.17.1)\nRequirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (3.1.6)\nRequirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (5.7.2)\nCollecting jupyter-lsp>=2.0.0 (from jupyterlab==4.3.2)\n  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (2.12.5)\nRequirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (2.27.3)\nRequirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (0.2.4)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (75.2.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (6.4.2)\nRequirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from jupyterlab==4.3.2) (5.7.1)\nRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (2.13.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.20.0)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm==0.10.0)\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (7.1.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.0->jupyterlab==4.3.2) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.28.0->jupyterlab==4.3.2) (0.14.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (1.1.0)\nRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (1.8.0)\nRequirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (7.34.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (8.6.3)\nRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (0.1.7)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (1.6.0)\nRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.3.2) (24.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyterlab==4.3.2) (3.0.2)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (23.1.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.12.0)\nRequirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.3)\nRequirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (6.4.5)\nRequirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (5.10.4)\nRequirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (7.7.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.21.1)\nRequirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.8.3)\nRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.18.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.8.0)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab==4.3.2) (4.3.8)\nRequirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (2.17.0)\nRequirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.12.0)\nRequirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (4.23.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (1.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm==0.10.0) (3.6.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.15.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.15.2) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.28.0->jupyterlab==4.3.2) (1.3.1)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (4.9.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab==4.3.2) (0.24.0)\nRequirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (3.3.0)\nRequirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.1.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.3.0)\nRequirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.4)\nRequirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (6.2.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.5.1)\nRequirement already satisfied: testpath in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.6.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (4.13.3)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.13)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.21.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm==0.10.0) (1.17.2)\nRequirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (21.2.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.8.4)\nRequirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.5.1)\nRequirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (3.0.0)\nRequirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.3.0)\nRequirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (24.11.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.3.2) (0.2.13)\nRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.17.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.6)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (0.5.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.22)\nRequirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.3.2) (2.9.0.20241206)\nDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hazm-0.10.0-py3-none-any.whl (892 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyterlab-4.3.2-py3-none-any.whl (11.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\nDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\nDownloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flashtext\n  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=064c43ea0a2ebfb3256b3f7b3877edafa89c263e46bb5c636f0775d56ea4d1f1\n  Stored in directory: /root/.cache/pip/wheels/49/20/47/f03dfa8a7239c54cbc44ff7389eefbf888d2c1873edaaec888\nSuccessfully built flashtext\nInstalling collected packages: portalocker, flashtext, sacrebleu, python-crfsuite, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, numpy, fsspec, async-lru, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, tokenizers, nvidia-cusolver-cu12, transformers, hazm, datasets, evaluate, accelerate, peft, jupyter-lsp, jupyterlab\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.1\n    Uninstalling tokenizers-0.21.1:\n      Successfully uninstalled tokenizers-0.21.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.6.0\n    Uninstalling datasets-3.6.0:\n      Successfully uninstalled datasets-3.6.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.5.2\n    Uninstalling accelerate-1.5.2:\n      Successfully uninstalled accelerate-1.5.2\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n  Attempting uninstall: jupyter-lsp\n    Found existing installation: jupyter-lsp 1.5.1\n    Uninstalling jupyter-lsp-1.5.1:\n      Successfully uninstalled jupyter-lsp-1.5.1\n  Attempting uninstall: jupyterlab\n    Found existing installation: jupyterlab 3.6.8\n    Uninstalling jupyterlab-3.6.8:\n      Successfully uninstalled jupyterlab-3.6.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\njupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, but you have jupyterlab 4.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.9.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.2.0 async-lru-2.0.5 datasets-3.1.0 evaluate-0.4.3 fasttext-wheel-0.9.2 flashtext-2.7 fsspec-2024.9.0 hazm-0.10.0 jupyter-lsp-2.2.5 jupyterlab-4.3.2 numpy-1.24.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.2 portalocker-2.0.0 python-crfsuite-0.9.11 sacrebleu-1.5.1 scipy-1.13.1 tokenizers-0.20.3 transformers-4.46.3\n","output_type":"stream"}],"execution_count":1},{"id":"5493be58","cell_type":"code","source":"# 📦 Imports\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom hazm import Normalizer\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n                          DataCollatorForSeq2Seq, Seq2SeqTrainingArguments,\n                          Seq2SeqTrainer)\n# TODO: add any other imports you need\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:53:56.809939Z","iopub.execute_input":"2025-05-28T11:53:56.810210Z","iopub.status.idle":"2025-05-28T11:54:38.111207Z","shell.execute_reply.started":"2025-05-28T11:53:56.810184Z","shell.execute_reply":"2025-05-28T11:54:38.110632Z"}},"outputs":[{"name":"stderr","text":"2025-05-28 11:54:20.451514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748433260.672000      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748433260.735498      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"a29e3ee9","cell_type":"markdown","source":"## 2 · Data Loading & Normalisation  \nPoint `FILE_PATH` to the Excel file of **ParsMap** dataset.\n1. Keep only the *informal* and *formal* columns.  \n2. Clean each sentence with `hazm.Normalizer`.  \n3. Create `train`, `validation`, and `test` splits (90 / 5 / 5 %).  \n","metadata":{}},{"id":"6a468e24","cell_type":"code","source":"# TODO ↓\nFILE_PATH = \"/kaggle/input/parsmap/ParsMap.xlsx\"\n\n# 1. Load the file\ndf = pd.read_excel(FILE_PATH)[['inFormalForm', 'formalForm']].rename(\n    columns={'inFormalForm':'input', 'formalForm':'target'}\n)\n\n# 2. Normalise\nnormalizer = Normalizer()\ndf['input']  = df['input'].astype(str).apply(normalizer.normalize)\ndf['target'] = df['target'].astype(str).apply(normalizer.normalize)\n\n# 3. Split to HF DatasetDict\nfull_ds = Dataset.from_pandas(df)\nfull_ds = full_ds.shuffle(seed=42)\nsplit_ds = full_ds.train_test_split(test_size=0.10, seed=42)\nval_test = split_ds['test'].train_test_split(test_size=0.50, seed=42)\ndataset = DatasetDict({'train': split_ds['train'],\n                       'validation': val_test['train'],\n                       'test': val_test['test']})\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:55:29.486255Z","iopub.execute_input":"2025-05-28T11:55:29.486999Z","iopub.status.idle":"2025-05-28T11:55:55.396723Z","shell.execute_reply.started":"2025-05-28T11:55:29.486973Z","shell.execute_reply":"2025-05-28T11:55:55.396113Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'target'],\n        num_rows: 45012\n    })\n    validation: Dataset({\n        features: ['input', 'target'],\n        num_rows: 2501\n    })\n    test: Dataset({\n        features: ['input', 'target'],\n        num_rows: 2501\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"4b80cf92","cell_type":"markdown","source":"## 3 · Token‑length Statistics  \nBefore padding/truncation, inspect sequence lengths to decide `max_length` for **inputs** and **targets**.  \nWrite a helper `length_stats()` that returns *min, max, mean, 95‑percentile*.  \n","metadata":{}},{"id":"54f2b7ed","cell_type":"code","source":"# TODO ↓\ntokenizer = AutoTokenizer.from_pretrained('google/mt5-base', use_fast=False)\n\ndef length_stats(texts):\n    \"\"\"Return descriptive statistics over tokenised length.\"\"\"\n    lengths = [len(tokenizer(text, truncation=False)['input_ids']) for text in texts]\n    return {\n        'mean': np.mean(lengths),\n        'median': np.median(lengths),\n        'max': np.max(lengths),\n        '25%': np.percentile(lengths, 25),\n        '75%': np.percentile(lengths, 75),\n    }\n\ninput_stats  = length_stats(dataset['train']['input'])\ntarget_stats = length_stats(dataset['train']['target'])\n\nprint('Input stats :', input_stats)\nprint('Target stats:', target_stats)\n\n# Decide sensible values\nMAX_SOURCE_LEN = 128  # TODO\nMAX_TARGET_LEN = 128  # TODO\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:09.827487Z","iopub.execute_input":"2025-05-28T11:56:09.828430Z","iopub.status.idle":"2025-05-28T11:56:21.905473Z","shell.execute_reply.started":"2025-05-28T11:56:09.828402Z","shell.execute_reply":"2025-05-28T11:56:21.904857Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61782f8783d54bd3af41dea5c219d64e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ccf64196ee4e2f838ad7a183d6706d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc3c2774b654b34b3df309777b70ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f757880f20c4b4c931fc1a49ed8580a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Input stats : {'mean': 22.689349506798187, 'median': 20.0, 'max': 146, '25%': 14.0, '75%': 28.0}\nTarget stats: {'mean': 24.65238158713232, 'median': 22.0, 'max': 150, '25%': 16.0, '75%': 30.0}\n","output_type":"stream"}],"execution_count":4},{"id":"4c68f9a4","cell_type":"markdown","source":"### Tokenisation function  \nComplete `preprocess_function` so that it returns `input_ids`, `attention_mask`, and `labels` truncated/padded to the lengths chosen above.","metadata":{}},{"id":"dbd59c3e","cell_type":"code","source":"# TODO ↓\ndef preprocess_function(batch):\n    model_inputs = tokenizer(\n        batch['input'],\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_SOURCE_LEN\n    )\n    labels = tokenizer(\n        batch['target'],\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_TARGET_LEN\n    )['input_ids']\n    model_inputs['labels'] = labels\n    return model_inputs\n\ntokenised_ds = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\ntokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:31.555334Z","iopub.execute_input":"2025-05-28T11:56:31.556020Z","iopub.status.idle":"2025-05-28T11:56:43.157126Z","shell.execute_reply.started":"2025-05-28T11:56:31.555987Z","shell.execute_reply":"2025-05-28T11:56:43.156407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c0d85001f14d46906256f5e548c454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2501 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798118dbd71a41509428e4be8ae235e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2501 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95a7d31231440b7a0bc2d3df8e5cefd"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 45012\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2501\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2501\n    })\n})"},"metadata":{}}],"execution_count":5},{"id":"76a82fb6","cell_type":"markdown","source":"## 4 · Model & LoRA Configuration  \nInstantiate *mT5‑base* and wrap it with **LoRA**.  \nRead the LoRA paper and, based on its insights and your available GPU resources, experiment with the *rank r*, `lora_alpha`, and target modules.”\n","metadata":{}},{"id":"1d5bd10d","cell_type":"code","source":"# TODO ↓\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,                     # rank\n    lora_alpha=32,           # scaling\n    target_modules=[\"q\", \"v\"],  # inject into query & value projections\n    lora_dropout=0.10,\n    bias='none',\n    task_type='SEQ_2_SEQ_LM'\n)\n\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-base')\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:57:48.788509Z","iopub.execute_input":"2025-05-28T11:57:48.789119Z","iopub.status.idle":"2025-05-28T11:57:59.142774Z","shell.execute_reply.started":"2025-05-28T11:57:48.789094Z","shell.execute_reply":"2025-05-28T11:57:59.141867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e32b9b36824fe4af4ac55f63fa1273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2e755416e384e33b676892897126168"}},"metadata":{}},{"name":"stdout","text":"trainable params: 884,736 || all params: 583,286,016 || trainable%: 0.1517\n","output_type":"stream"}],"execution_count":6},{"id":"75687ce8","cell_type":"markdown","source":"## 5 · Fine‑tuning  \nDefine `Seq2SeqTrainingArguments` and train for **3 epochs**  \nLog training loss and evaluate on the validation set each epoch.  \n","metadata":{}},{"id":"2b2046f9","cell_type":"code","source":"# TODO ↓\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mt5-persian-formalizer\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=4e-4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LEN,\n    fp16=True,\n    report_to=\"none\"\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest')\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenised_ds['train'],\n    eval_dataset=tokenised_ds['validation'],\n    data_collator=data_collator\n)\n\n# 🚀 Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:59:48.348534Z","iopub.execute_input":"2025-05-28T11:59:48.348826Z","iopub.status.idle":"2025-05-28T16:18:50.862130Z","shell.execute_reply.started":"2025-05-28T11:59:48.348806Z","shell.execute_reply":"2025-05-28T16:18:50.861523Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16881' max='16881' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16881/16881 4:18:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.257800</td>\n      <td>0.157878</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.202000</td>\n      <td>0.132445</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.183800</td>\n      <td>0.122864</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=16881, training_loss=0.4402227787869525, metrics={'train_runtime': 15541.02, 'train_samples_per_second': 8.689, 'train_steps_per_second': 1.086, 'total_flos': 4.057035225012634e+16, 'train_loss': 0.4402227787869525, 'epoch': 3.0})"},"metadata":{}}],"execution_count":9},{"id":"784b8efd","cell_type":"markdown","source":"## 6 · Inference  \nGenerate the *formal* version of **5 custom informal sentences** using **greedy decoding** *and* your `MAX_TARGET_LEN`.  \n","metadata":{}},{"id":"903b6b60","cell_type":"code","source":"# TODO ↓\nexample_inputs = [\n    \"واسه چی اینقدر دیر اومدی؟\",\n    \"من امروز نتونستم سر وقت برسم، متأسفم.\",\n    \"این کار رو چطوری باید انجام بدم؟\",\n    \"می‌خوای با هم بریم کافی‌شاپ؟\",\n    \"دیروز فیلم جدیدو دیدی؟\"\n]\n\n# Greedy decoding\nfor inp in example_inputs:\n    # tokenize\n    inputs = tokenizer(\n        inp,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=MAX_SOURCE_LEN\n    )\n    # move each tensor to the model’s device (e.g. cuda:0)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # now generate\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs.get(\"attention_mask\"),\n        max_length=MAX_TARGET_LEN\n    )\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(f\"IN : {inp}\\nOUT: {decoded}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:25:03.387132Z","iopub.execute_input":"2025-05-28T16:25:03.387402Z","iopub.status.idle":"2025-05-28T16:25:05.223911Z","shell.execute_reply.started":"2025-05-28T16:25:03.387384Z","shell.execute_reply":"2025-05-28T16:25:05.223189Z"}},"outputs":[{"name":"stdout","text":"IN : واسه چی اینقدر دیر اومدی؟\nOUT: برای چه این قدر دیر آمده ای؟\n\nIN : من امروز نتونستم سر وقت برسم، متأسفم.\nOUT: من امروز نتوانم سر وقت برسم، متأسف هستم.\n\nIN : این کار رو چطوری باید انجام بدم؟\nOUT: این کار را چطوری باید انجام بدهم؟\n\nIN : می‌خوای با هم بریم کافی‌شاپ؟\nOUT: می خواهی با هم به کافی شاپ برویم.\n\nIN : دیروز فیلم جدیدو دیدی؟\nOUT: دیروز فیلم جدید را دیدی.\n\n","output_type":"stream"}],"execution_count":14},{"id":"17e604cf","cell_type":"markdown","source":"## 7 · Evaluation  \nCompute **BLEU** on the *test* split and report **perplexity** on *validation*.  \nExplain briefly what each metric captures for this task.  \n","metadata":{}},{"id":"bf7292d5","cell_type":"code","source":"# TODO ↓\nimport evaluate, math\n\nbleu = evaluate.load('sacrebleu')\n\n# 1. Generate predictions\npreds, refs = [], []\nfor example in dataset['test']:\n    inp, tgt = example['input'], example['target']\n    encoded = tokenizer(\n        inp,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=MAX_SOURCE_LEN\n    )\n    encoded = {k: v.to(model.device) for k, v in encoded.items()}\n    out = model.generate(**encoded, max_length=MAX_TARGET_LEN)\n    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n    preds.append(pred)\n    refs.append([tgt])\n\nbleu_score = bleu.compute(predictions=preds, references=refs)\nprint(f\"Test BLEU: {bleu_score['score']:.2f}\")\n\n# 2. Compute perplexity\neval_results = trainer.evaluate(tokenised_ds['test'])\nperplexity = math.exp(eval_results['eval_loss'])\nprint(f\"Test Perplexity: {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:25:33.259266Z","iopub.execute_input":"2025-05-28T16:25:33.260096Z","iopub.status.idle":"2025-05-28T16:54:18.293322Z","shell.execute_reply.started":"2025-05-28T16:25:33.260073Z","shell.execute_reply":"2025-05-28T16:54:18.292737Z"}},"outputs":[{"name":"stdout","text":"Test BLEU: 38.52\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 02:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Perplexity: 1.13\n","output_type":"stream"}],"execution_count":15},{"id":"907f9709-eada-438b-943f-2f0301b653d8","cell_type":"markdown","source":"### Evaluation Metrics\n\n**BLEU Score**  \nBLEU (Bilingual Evaluation Understudy) measures the n-gram overlap between your model’s generated “formal” sentences and the ground-truth formal references. In this style-transfer task, a higher BLEU indicates that the model’s rephrasings closely match human-written formal versions in terms of word choice, phrase structure, and overall lexical fidelity.\n\n**Perplexity**  \nPerplexity is computed as the exponential of the model’s cross-entropy loss on the test set. It captures how “surprised” the model is, on average, when predicting each next token. Lower perplexity means the model has learned the formal style’s probability distribution well and finds the generation task more predictable, reflecting stronger overall language modeling of the target register.\n","metadata":{}},{"id":"4f3d1220","cell_type":"markdown","source":"## 8 · Stochastic Decoding & Diversity Analysis  \n\nRead *Holtzman et al. 2020* — *The Curious Case of Neural Text Degeneration* — to understand how different **stochastic decoding** strategies (like temperature, top‑k, and top‑p sampling) can lead to generating multiple diverse outputs from the same input prompt.\n\nImplement these decoding strategies and experiment with several input examples to observe how the outputs vary.","metadata":{}},{"id":"ef64fad3","cell_type":"code","source":"# TODO ↓\ndef sample_outputs(\n    prompt: str,\n    num_return_sequences: int = 5,\n    temperature: float = 0.7,\n    top_k: int = 50,\n    top_p: float = 1.0\n):\n    \"\"\"Generate diverse outputs from the fine-tuned model.\"\"\"\n    # Tokenize\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_SOURCE_LEN\n    )\n    # Move inputs to the same device as the model\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Sampling generation\n    outputs = model.generate(\n        **inputs,\n        max_length=MAX_TARGET_LEN,\n        do_sample=True,\n        num_return_sequences=num_return_sequences,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    # Decode and return\n    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n\nprompt = \"تو مطمئنی که بابا بلده گره دوتائی به کفشم بزنه وقتی که من صبحها میخوام برم مدرسه؟\"\nsamples = sample_outputs(prompt, num_return_sequences=5, temperature=0.9, top_p=0.95)\nprint(*samples, sep='\\n---\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T16:55:52.411549Z","iopub.execute_input":"2025-05-28T16:55:52.411868Z","iopub.status.idle":"2025-05-28T16:55:54.055212Z","shell.execute_reply.started":"2025-05-28T16:55:52.411847Z","shell.execute_reply":"2025-05-28T16:55:54.054560Z"}},"outputs":[{"name":"stdout","text":"تو مطمئن هستی که بابا بلده گره دوتایی به کفشم بزن وقتی که من صبح ها می خواهم به مدرسه برم؟\n---\nتو مطمئن هستی که بابا بلده گره دوتایی به کفشم بزند وقتی که من صبح ها می خواهم به مدرسه برم؟\n---\nتو مطمئن هستی که بابا بلده گره دوتایی به کفش هم بزند وقتی که من صبح ها می خواهم به مدرسه برم؟\n---\nتو مطمئن هستی که بابا بلده گره دوتائی به کفش هم بزند وقتی که من صبح ها می خواهم برویم، مدرسه؟\n---\nتو مطمئنی که بابا بلده گره دوتایی به کفش هم بزن وقتی که من صبح ها می خواهم برم مدرسه؟\n","output_type":"stream"}],"execution_count":17},{"id":"13f1b754","cell_type":"markdown","source":"## 9 · Discussion \n\n1. How did LoRA hyper‑parameters influence training stability or performance?  \n2. **Deterministic vs. Stochastic Decoding**  \n   Briefly explain what deterministic decoding (e.g. greedy search, beam search) and stochastic decoding (e.g. temperature sampling, top‑k/top‑p nucleus sampling) mean, drawing on Holtzman et al. 2020, *The Curious Case of Neural Text Degeneration*.\n3. Suggest one improvement to the data or model that could further boost formalisation quality.  \n","metadata":{}},{"id":"850e1a36-890f-4ee7-bc55-8397a3d19d70","cell_type":"markdown","source":"## 9 · Discussion\n\n1. **LoRA Hyper-Parameter Influence**  \n   - **Rank (r)**: Controls the number of adapter parameters. A moderate rank (e.g. 8) balanced adaptation capacity with efficiency—lower ranks reduced memory use but sometimes slowed convergence; higher ranks improved final BLEU at the cost of more trainable parameters.  \n   - **Alpha (lora_alpha)**: Scales the adapter updates. A larger α (e.g. 32) amplified adapter gradients, smoothing training and helping the model adapt quickly without destabilizing pre-trained weights.  \n   - **Dropout**: Applying dropout (e.g. 0.1) in the adapter layers regularized fine-tuning, preventing overfitting on our relatively small ParsMap corpus and improving generalization.  \n   - **Target Modules**: Injecting LoRA only into the query and value projection matrices focused capacity on the most expressive subspaces, yielding more stable and efficient learning compared to tuning all model layers.\n\n2. **Deterministic vs. Stochastic Decoding**  \n   - **Deterministic decoding** (greedy search, beam search) always picks the highest-probability token (or sequence) at each step. It produces reproducible, high-likelihood outputs but often “safe,” generic text.  \n   - **Stochastic decoding** (temperature sampling, top-k/top-p nucleus sampling) samples from the model’s probability distribution, allowing randomness and greater diversity. Temperature scales the logits before sampling, while top-k/top-p truncate low-probability tokens. As Holtzman et al. (2020) discuss in *The Curious Case of Neural Text Degeneration*, naive sampling can lead to repetitive or incoherent text, and nucleus (top-p) sampling effectively balances coherence with diversity by dynamically selecting the most probable subset of tokens.\n\n3. **Suggested Improvement**  \n   **Back-translation data augmentation**: Translate existing formal sentences back into informal variants (using a reverse informalization model), then pair these synthetic informal–formal examples with your real data. This enlarges and diversifies the parallel corpus, helping the model learn more robust formality mappings and reducing overfitting.  \n","metadata":{}},{"id":"2da6b808","cell_type":"markdown","source":"---\n\n### Submission Checklist ✅\n\n- [ ] All `TODO` blocks completed.  \n- [ ] Notebook runs end‑to‑end without errors (`Runtime ⇾ Restart & Run All`).  \n- [ ] Answers written in the *Discussion* section.  \n\nGood luck, and have fun experimenting! ✨\n","metadata":{}},{"id":"6d0bac44","cell_type":"markdown","source":"","metadata":{}}]}
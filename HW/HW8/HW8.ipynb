{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aeea70e1",
      "metadata": {
        "id": "aeea70e1"
      },
      "source": [
        "First & Second Name: Seyyed Amirmahdi\n",
        "\n",
        "Student ID: 401102015\n",
        "\n",
        "Note I: Each section is worth 50 points. Each question is worth 5 points, and your implementation is worth 30 points in total. (5 × 4 + 30 = 50) × 2 = 100.\n",
        "\n",
        "Note II: For each section, your model (agent) can converge. By achieving convergence in each section, you will earn an additional **5 points** per section. Alternatively, if your agent demonstrates an incremental average return, that will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4830028d",
      "metadata": {
        "id": "4830028d"
      },
      "source": [
        "# Section One"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7879bfaa",
      "metadata": {
        "id": "7879bfaa"
      },
      "source": [
        "### Develop the model of nueral network for training agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6081eb2f",
      "metadata": {
        "id": "6081eb2f"
      },
      "source": [
        "In this section, you’ll embark on implementing a Deep Q-Network (DQN) agent from scratch, one of the fundamental algorithms in deep reinforcement learning. DQN revolutionized the field by successfully combining deep neural networks with Q-learning, enabling agents to learn directly from high-dimensional sensory inputs. You’ll build each component systematically - starting with the neural network architecture that approximates Q-values, then implementing the experience replay buffer that breaks correlations in training data, followed by the epsilon-greedy policy for balancing exploration and exploitation, and finally bringing it all together in the DQN agent class. This hands-on implementation will deepen your understanding of how value-based reinforcement learning algorithms work under the hood.\n",
        "Below is the schema for DQN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8638979",
      "metadata": {
        "id": "d8638979"
      },
      "source": [
        "![image-3](https://www.researchgate.net/publication/344238597/figure/fig4/AS:935663859937283@1600091056689/DQN-training-process.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdc8141",
      "metadata": {
        "id": "ccdc8141"
      },
      "source": [
        "The CartPole-v1 environment serves as your testing ground - a classic control problem where an agent must balance a pole on a moving cart. Through this implementation, you’ll gain practical experience with key concepts like target networks for stable learning, batch sampling from replay memory, and the temporal difference learning update rule. Pay special attention to how each component interacts with others: how the neural network processes states to output Q-values, how the replay buffer stores and samples experiences, and how the epsilon-greedy policy uses these Q-values to select actions. The visualization at the end will show your agent’s learning progress across multiple random seeds, demonstrating the effectiveness of your implementation. The conceptual questions following the implementation will help solidify your understanding of where DQN fits in the broader landscape of reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b7cc93",
      "metadata": {
        "id": "10b7cc93"
      },
      "source": [
        "# Implement Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5d3d375c",
      "metadata": {
        "id": "5d3d375c"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import collections\n",
        "\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "class FCModel(nn.Module):\n",
        "    \"\"\" Implement a dense nn for using on QNetwork\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 1) Creat a nueral network (dense) for getting with input_size and return output_size\n",
        "        super().__init__()\n",
        "        # two hidden layers of size 64, with ReLU\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 2) Implement forward function\n",
        "        return self.net(inputs)\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "    pass\n",
        "\n",
        "# QNetwork class\n",
        "class QNetwork:\n",
        "    \"\"\" Implement QNetwork for estimate q-values\"\"\"\n",
        "    def __init__(self, env, lr):\n",
        "\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 3) Implement optimizer\n",
        "        # 4) Implement suitable lr\n",
        "        # 5) and self.net which contain a FCModel in it\n",
        "\n",
        "        input_size = env.observation_space.shape[0]\n",
        "        output_size = env.action_space.n\n",
        "        self.net = FCModel(input_size, output_size)\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d85c535",
      "metadata": {
        "id": "6d85c535"
      },
      "source": [
        "### Develop the replay buffer for adding experiences in buffers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bd7b7838",
      "metadata": {
        "id": "bd7b7838"
      },
      "outputs": [],
      "source": [
        "# Memory/Replay Buffer class\n",
        "class ReplayMemory:\n",
        "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.memory = collections.deque(maxlen=memory_size)\n",
        "        self.env = env\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 6) Impelement sample batch function to return sample from replay buffer with giben batch size\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states = np.stack([t.state      for t in batch], axis=0)\n",
        "        actions = np.array([t.action     for t in batch], dtype=np.int64)\n",
        "        rewards = np.array([t.reward     for t in batch], dtype=np.float32)\n",
        "        next_states = np.stack([t.next_state for t in batch], axis=0)\n",
        "        return states, actions, rewards, next_states\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def burn_in_memory(self):\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 7) Fill the memory with random transitions for burn_in steps\n",
        "        # the storing should be with Transition nametuple which implemented above\n",
        "        state, _ = self.env.reset()\n",
        "        for _ in range(self.burn_in):\n",
        "            action = self.env.action_space.sample()\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            # store the transition\n",
        "            self.append(Transition(state, action, next_state, reward))\n",
        "            # move to the next state (reset if episode ended)\n",
        "            if terminated or truncated:\n",
        "                state, _ = self.env.reset()\n",
        "            else:\n",
        "                state = next_state\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5043cccc",
      "metadata": {
        "id": "5043cccc"
      },
      "source": [
        "### Policy module for selecting best action in given states. (Note: you can add the update_epsilone function to update the epsilone for each iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "264ca1b5",
      "metadata": {
        "id": "264ca1b5"
      },
      "outputs": [],
      "source": [
        "class Policy:\n",
        "    def __init__(self, env, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def select_action(self, state, q_network, training=True):\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 8) implement select action with epsilon greedy\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # explore\n",
        "            action = self.env.action_space.sample()\n",
        "            return torch.tensor([[action]], dtype=torch.long)\n",
        "        else:\n",
        "            # exploit\n",
        "            with torch.no_grad():\n",
        "                q_values = q_network.net(state)\n",
        "                action = q_values.argmax(dim=1).view(1, 1)\n",
        "            return action\n",
        "        # comment the line below to test your code\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "        return torch.argmax(q_network.net(state)).view(1, 1)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        \"\"\"Decay epsilon\"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b241ec",
      "metadata": {
        "id": "c2b241ec"
      },
      "source": [
        "### Develop Agent for DQN alghorithm. This module is reponsbile for training, testing, fill the buffer and optimize the learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "269dfbef",
      "metadata": {
        "id": "269dfbef"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, env_name, lr=1e-4, gamma=0.99, batch_size=128,\n",
        "                 target_update=10, memory_size=50000, burn_in=10000,\n",
        "                 epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995):\n",
        "\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 8) inital all element you implemented before like replay buffer, policy etc.\n",
        "        self.env = gym.make(env_name)\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.burn_in = burn_in\n",
        "\n",
        "        # replay buffer\n",
        "        self.memory = ReplayMemory(self.env, memory_size=memory_size, burn_in=burn_in)\n",
        "        # epsilon-greedy policy\n",
        "        self.policy = Policy(self.env, epsilon_start=epsilon_start, epsilon_end=epsilon_end, epsilon_decay=epsilon_decay)\n",
        "\n",
        "        # online Q-network and target Q-network\n",
        "        self.policy_net = QNetwork(self.env, lr)\n",
        "        self.target_net = QNetwork(self.env, lr)\n",
        "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
        "        self.target_net.net.eval()\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the agent for one episode\"\"\"\n",
        "        if len(self.memory) < self.burn_in:\n",
        "            self.memory.burn_in_memory()\n",
        "\n",
        "        state, _ = self.env.reset()\n",
        "\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 9) Complete training function, you should use optimize_model function\n",
        "        episode_reward = 0.0\n",
        "        episode_loss = 0.0\n",
        "        steps = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            action_tensor = self.policy.select_action(state_tensor, self.policy_net, training=True)\n",
        "            action = action_tensor.item()\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if not done:\n",
        "                next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                next_state_tensor = None\n",
        "\n",
        "            # reward tensor\n",
        "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
        "\n",
        "            # store\n",
        "            self.memory.append(Transition(state_tensor, action_tensor, next_state_tensor, reward_tensor))\n",
        "\n",
        "            # one gradient‐step\n",
        "            loss = self._optimize_model()\n",
        "            episode_loss += loss\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "            state = next_state\n",
        "\n",
        "            if steps % self.target_update == 0:\n",
        "                self.target_net.net.load_state_dict(\n",
        "                    self.policy_net.net.state_dict()\n",
        "                )\n",
        "\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "        # Update epsilon\n",
        "        self.policy.update_epsilon()\n",
        "        return episode_reward, episode_loss / steps if steps > 0 else 0, steps\n",
        "\n",
        "    def _optimize_model(self):\n",
        "        \"\"\"Perform one step of optimization\"\"\"\n",
        "        transitions = self.memory.sample_batch(self.batch_size)\n",
        "        batch = Transition(*transitions)  # This is correct, no change needed here\n",
        "\n",
        "        # Compute a mask of non-final states\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "                                      dtype=torch.bool)\n",
        "\n",
        "        # Convert the list of next states to tensor (for non-final states only)\n",
        "        non_final_next_states = torch.stack([torch.tensor(s, dtype=torch.float32)\n",
        "                                            for s in batch.next_state if s is not None])\n",
        "\n",
        "        # Convert all the other elements (states, actions, rewards) to tensors\n",
        "        state_batch = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.state]).view(-1, 4)  # Ensuring correct shape [batch_size, 4]\n",
        "\n",
        "        # Ensure actions and rewards are converted to tensors with the correct shape\n",
        "        action_batch = torch.tensor(batch.action, dtype=torch.long).view(-1, 1)  # Ensure shape [batch_size, 1]\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32).view(-1, 1)  # Ensure shape [batch_size, 1]\n",
        "\n",
        "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "        state_action_values = self.policy_net.net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states\n",
        "        next_state_values = torch.zeros(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            # Next state values for non-terminal states\n",
        "            next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0]\n",
        "\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        # Compute loss\n",
        "        criterion = nn.MSELoss()\n",
        "        loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.policy_net.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        self.policy_net.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def test(self, render=False):\n",
        "        \"\"\"Test the agent on one episode\"\"\"\n",
        "        state, _ = self.env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        #####################################################################\n",
        "        # TODO:\n",
        "        # 10) Complete training function\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            action_tensor = self.policy.select_action(state_tensor, self.policy_net, training=False)\n",
        "            action = action_tensor.item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            total_reward += reward\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        #raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "        #####################################################################\n",
        "\n",
        "        return total_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cb96f8dc",
      "metadata": {
        "id": "cb96f8dc",
        "outputId": "2285d7a0-00f4-45f9-8d0e-a9615937c9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x128 and 4x64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-28ffa0874a3b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5e96a126d343>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# one gradient‐step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mepisode_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5e96a126d343>\u001b[0m in \u001b[0;36m_optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# Next state values for non-terminal states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mnext_state_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Compute the expected Q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-14b5a2519da1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# TODO:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# 2) Implement forward function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# comment the line below to test your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#raise NotImplementedError(\" No filter has implemented to the images\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x128 and 4x64)"
          ]
        }
      ],
      "source": [
        "# Note: Feel free to adjust any hyperparameter you want for training. Except these: seed, env_name, state_space_size\n",
        "if __name__ == \"__main__\":\n",
        "    env_name = 'CartPole-v1'\n",
        "    num_episodes_train = 200\n",
        "    num_episodes_test = 20\n",
        "    learning_rate = 5e-4\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    action_space_size = env.action_space.n\n",
        "    state_space_size = 4  # CartPole state space size is 4\n",
        "\n",
        "    num_seeds = 5\n",
        "    l = num_episodes_train // 10\n",
        "    res = np.zeros((num_seeds, l))\n",
        "    gamma = 0.99\n",
        "\n",
        "    for i in tqdm.tqdm(range(num_seeds)):\n",
        "        reward_means = []\n",
        "        agent = DQNAgent(env_name, lr=learning_rate, gamma=gamma)\n",
        "\n",
        "        for m in range(num_episodes_train):\n",
        "            episode_reward, episode_loss, steps = agent.train()\n",
        "\n",
        "            if m % 10 == 0:\n",
        "                print(f\"Episode: {m}\")\n",
        "\n",
        "                G = np.zeros(num_episodes_test)\n",
        "                for k in range(num_episodes_test):\n",
        "                    g = agent.test()\n",
        "                    G[k] = g\n",
        "\n",
        "                reward_mean = G.mean()\n",
        "                reward_sd = G.std()\n",
        "                print(f\"The test reward for episode {m} is {reward_mean} with a standard deviation of {reward_sd}.\")\n",
        "                reward_means.append(reward_mean)\n",
        "\n",
        "        res[i] = np.array(reward_means)\n",
        "\n",
        "    ks = np.arange(l) * 10\n",
        "    avs = np.mean(res, axis=0)\n",
        "    maxs = np.max(res, axis=0)\n",
        "    mins = np.min(res, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
        "    plt.plot(ks, avs, '-o', markersize=1)\n",
        "\n",
        "    plt.xlabel('Episode', fontsize=15)\n",
        "    plt.ylabel('Avg. Return', fontsize=15)\n",
        "    plt.title('DQN Performance on CartPole-v1')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('dqn_performance.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51bbab75",
      "metadata": {
        "id": "51bbab75"
      },
      "source": [
        "## Answer these questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905e1ffd",
      "metadata": {
        "id": "905e1ffd"
      },
      "source": [
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58291e84",
      "metadata": {
        "id": "58291e84"
      },
      "source": [
        "Q1) According given image, explain the value based, policy based apporches in MODEL FREE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c903cc2d",
      "metadata": {
        "id": "c903cc2d"
      },
      "source": [
        "Q2) The DQN you implemented is consider which one of those? why? what is rule of the Policy Class you implemented?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0204f99d",
      "metadata": {
        "id": "0204f99d"
      },
      "source": [
        "Q3) Explain the rule of Replay Buffer in algorithm. (why we should use this?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab38138b",
      "metadata": {
        "id": "ab38138b"
      },
      "source": [
        "Q4) Explain the rule of epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82b2ee6",
      "metadata": {
        "id": "d82b2ee6"
      },
      "source": [
        "# Section Two"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7108f6",
      "metadata": {
        "id": "ff7108f6"
      },
      "source": [
        "Section Two challenges you to implement the Soft Actor-Critic (SAC) algorithm, representing a significant shift from the value-based approach of DQN to a state-of-the-art actor-critic method. SAC belongs to the family of maximum entropy reinforcement learning algorithms, which explicitly balance reward maximization with maintaining stochastic policies for better exploration. Unlike DQN’s discrete action selection, SAC can naturally handle continuous action spaces while incorporating entropy regularization to encourage exploration and prevent premature convergence to suboptimal policies. You’ll need to architect multiple neural networks - an actor network that outputs actions, critic networks that evaluate state-action pairs, and potentially a value network depending on your implementation choice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f87399",
      "metadata": {
        "id": "25f87399"
      },
      "source": [
        "Implementing SAC requires understanding several advanced concepts including the reparameterization trick for gradient computation through stochastic nodes, soft value functions that incorporate entropy bonuses, and the careful orchestration of multiple function approximators. The algorithm’s elegance lies in its ability to automatically tune the temperature parameter that balances exploration and exploitation. As you build your implementation, consider how SAC differs fundamentally from DQN in its approach to the exploration-exploitation trade-off and its ability to maintain a stochastic policy throughout training. The comparison questions at the end will help you articulate these differences and understand when each algorithm might be preferred. Your implementation will again be tested on CartPole-v1, allowing direct comparison with your DQN results and highlighting the distinct characteristics of policy-based versus value-based methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89dc7cbe",
      "metadata": {
        "id": "89dc7cbe"
      },
      "source": [
        "### Implement SAC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86171200",
      "metadata": {
        "id": "86171200"
      },
      "source": [
        "Implement SAC with (Cart-pole) enviroment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85de8d3f",
      "metadata": {
        "id": "85de8d3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "#####################################################################\n",
        "# TODO:\n",
        "# 10) Implement whole SAC. you can inspire by DQN.\n",
        "raise NotImplementedError(\" No filter has implemented to the images\")\n",
        "#####################################################################\n",
        "\n",
        "# Below is how you modules can be\n",
        "# Feels free to adjsut as you like\n",
        "for run in range(n_runs):\n",
        "    print(f\"Run {run+1}/{n_runs}\")\n",
        "    seed = 100 + run\n",
        "    env = gym.make('CartPole-v1')\n",
        "    set_seed(seed, env)\n",
        "    policy = Policy()\n",
        "    value_net = ValueNetwork()\n",
        "    optimizer_policy = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "    optimizer_value = optim.Adam(value_net.parameters(), lr=1e-2)\n",
        "    scores = A2C(policy, value_net, env, optimizer_policy, optimizer_value, n_episodes=n_episodes)\n",
        "    all_scores.append(scores)\n",
        "    env.close()\n",
        "\n",
        "all_scores = np.array(all_scores)\n",
        "mean_scores = np.mean(all_scores, axis=0)\n",
        "std_scores = np.std(all_scores, axis=0)\n",
        "conf_interval = 1.96 * std_scores / np.sqrt(n_runs)\n",
        "\n",
        "episodes = np.arange(1, n_episodes + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(episodes, mean_scores, label='Mean Reward')\n",
        "plt.fill_between(episodes, mean_scores - conf_interval, mean_scores + conf_interval, alpha=0.3, color='blue', label='95% Confidence Interval')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('A2C: CartPole-v1\\nMean and 95% Confidence Interval over 5 runs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ebaa32",
      "metadata": {
        "id": "24ebaa32"
      },
      "source": [
        "Q1) Plot the average return with 5 different seed, mean and std."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabc2127",
      "metadata": {
        "id": "fabc2127"
      },
      "source": [
        "Q2) Explain the SAC algorithm; what is different between SAC and DQN?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246b0f9b",
      "metadata": {
        "id": "246b0f9b"
      },
      "source": [
        "Q3) According to the diagram which is given in the previous question, how is SAC categorized?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2c40b4",
      "metadata": {
        "id": "ce2c40b4"
      },
      "source": [
        "Q4) Explain each components in SAC, and functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32b1376",
      "metadata": {
        "id": "b32b1376"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
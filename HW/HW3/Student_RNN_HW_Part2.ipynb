{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b2f2b9",
   "metadata": {},
   "source": [
    "# LSTM-based Seq2Seq Model for Abstractive Summarization\n",
    "\n",
    "You can ask your questions in Telegram : @FatemehNikkhoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad30cc",
   "metadata": {},
   "source": [
    "Name = \"\"\n",
    "\n",
    "StudentId = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b2e4e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d13e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e4a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ca28f",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb10e6",
   "metadata": {},
   "source": [
    "# Extreme Summarization (XSum) Dataset\n",
    "\n",
    "The **XSUM** dataset is designed for the task of extreme summarization, where the goal is to generate a single-sentence summary for a news article. \n",
    "\n",
    "### Features:\n",
    "- **document:** The input news article.\n",
    "- **summary:** A one-sentence summary of the article.\n",
    "- **id:** A unique BBC ID for each article.\n",
    "\n",
    "For more details and to explore the dataset, you can visit the official [Hugging Face XSUM page](https://huggingface.co/datasets/xsum).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb7f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XSUM dataset...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the XSUM dataset\n",
    "print(\"Loading XSUM dataset...\")\n",
    "\n",
    "# Load each split using slice syntax\n",
    "raw_datasets = {\n",
    "    \"train\": load_dataset(\"xsum\", split=\"train[:500]\"),\n",
    "    \"validation\": load_dataset(\"xsum\", split=\"validation[:100]\"),\n",
    "    \"test\": load_dataset(\"xsum\", split=\"test[:100]\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ff575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 500\n",
      "validation size: 100\n",
      "test size: 100\n",
      "Sample from random index: 437\n",
      "\n",
      "document: Eight hundred of the plants, as well as electrical equipment, were seized at a workshop in Millisle on Friday.\n",
      "Police said it was one of the biggest and most sophisticated production operations they had uncovered recently.\n",
      "Insp Andy Dunlop said there were \"young plants, the remnants of a previous harvest, space for drying the product and space for waste products\".\n",
      "He added: \"No arrests were made this morning but our enquiries will be continuing over the coming days.\"\n",
      "\n",
      "summary: Cannabis plants worth an estimated £400,000 have been seized by police  in County Down.\n",
      "\n",
      "id: 30452755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Inspection \n",
    "\n",
    "# Inspect the dataset size\n",
    "for split, data in raw_datasets.items():\n",
    "    print(f\"{split} size: {len(data)}\")\n",
    "\n",
    "# Inspect a random sample of the train dataset\n",
    "train_len = len(raw_datasets['train'])\n",
    "# Select a random index between 0 and train_len - 1\n",
    "random_index = random.randint(0, train_len  - 1)\n",
    "print(f\"Sample from random index: {random_index}\\n\")\n",
    "for key in raw_datasets['train'][random_index]:\n",
    "    print(f\"{key}: {raw_datasets['train'][random_index][key]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae830f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "### Question:\n",
    "- What is the role of a tokenizer in Natural Language Processing (NLP)?\n",
    "- What does it mean to \"tokenize\" text, and why is this step necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263686e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "# Apply tokenization on the 'document' (news article) and 'summary' (highlight).\n",
    "def tokenize_function(example, tokenizer):\n",
    "    \"\"\"\n",
    "    This function takes a batch of example and applies tokenization using the provided tokenizer.\n",
    "    \n",
    "    Args:\n",
    "    example (dict): A dictionary containing text data with keys like \"document\" and \"summary\".\n",
    "    tokenizer: A tokenizer instance (e.g., from `torchtext` or `transformers`).\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing tokenized inputs and target sequences with keys 'input_ids' and 'target_ids'.\n",
    "    \"\"\"\n",
    "    # TODO: Apply tokenization\n",
    "    # Place your code here\n",
    "    # inputs  = ... # Tokenizing the article (input)\n",
    "    # targets = ... # Tokenizing the summary (target)\n",
    "    return {\"input_ids\": inputs, \"target_ids\": targets}\n",
    "\n",
    "\n",
    "# Tokenizer (using basic English tokenizer)\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Basic word-level tokenization\n",
    "# Applying the tokenizer function to the dataset\n",
    "tokenized_datasets = {\n",
    "    split: raw_datasets[split].map(lambda example: tokenize_function(example, tokenizer))\n",
    "    for split in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "\n",
    "# TODO: Inspect a sample of tokenized_datasets['train'] to better understand the results\n",
    "# Print the keys and values of the sample at the random_index that was calculated earlier\n",
    "# Loop through the keys of the random sample and print both key and value\n",
    "# Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f0397",
   "metadata": {},
   "source": [
    "# Build Vocabulary\n",
    "\n",
    "In NLP tasks, the vocabulary maps each token (word) to a unique integer ID.\n",
    "\n",
    "### Question:\n",
    "- What are the special characters `\"<unk>\"` and `\"<pad>\"` used for in vocabulary generation?\n",
    "- Why should we build the vocabulary using only the training data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build Vocabulary\n",
    "def build_vocab(texts, tokenizer):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the provided raw text data.\n",
    "    The vocabulary maps each token (word) to a unique integer ID.\n",
    "    Special tokens like <unk> (unknown words) and <pad> (padding) are included.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of raw text data (e.g., articles).\n",
    "        tokenizer: The tokenizer function to tokenize the texts.\n",
    "\n",
    "    Returns:\n",
    "        vocab: A vocabulary object that maps each token to an integer ID.\n",
    "    \"\"\"\n",
    "    # Using build_vocab_from_iterator to create vocabulary from tokenized data\n",
    "    # Special tokens <unk> and <pad> are added during vocabulary creation\n",
    "    return build_vocab_from_iterator(map(tokenizer, texts), specials=[\"<unk>\", \"<pad>\"])\n",
    "\n",
    "\n",
    "# TODO: Build the vocabulary from the training data considering both 'documents' and 'summary'\n",
    "# Place your code here\n",
    "# vocab = ...\n",
    "\n",
    "# Inspecting the vocabulary:\n",
    "# Get the length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "# To better understand the vocabulary, let's print the first 10 tokens and their corresponding IDs.\n",
    "# This helps in ensuring that the special tokens are included, and the vocabulary is mapped correctly.\n",
    "print(\"Sample tokens and their corresponding IDs:\")\n",
    "for token in list(vocab.get_itos())[:10]:\n",
    "    print(token, vocab[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17032849",
   "metadata": {},
   "source": [
    "## Padding Function\n",
    "\n",
    "### Question: \n",
    "- Why is padding important in data preprocessing for NLP tasks, and why should we do it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Padding function (modified to accept token IDs)\n",
    "# Constants\n",
    "MAX_LENGTH = 128#512  # Maximum sequence length\n",
    "PAD_IDX = vocab[\"<pad>\"]  # Padding token index\n",
    "UNK_IDX = vocab[\"<unk>\"]  # Unknown token index\n",
    "\n",
    "def pad_to_max_length(seq, max_length=MAX_LENGTH, pad_idx=PAD_IDX):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence of token IDs to a fixed maximum length.\n",
    "    \n",
    "    Args:\n",
    "        seq (list): Sequence of token IDs.\n",
    "        max_length (int): Target sequence length.\n",
    "        pad_idx (int): Padding token ID.\n",
    "\n",
    "    Returns:\n",
    "        list: Padded/truncated sequence of token IDs.\n",
    "    \"\"\"\n",
    "    return seq + [pad_idx] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length]\n",
    "\n",
    "# 5. Sequence processing function (ensure tokenization and conversion to token IDs)\n",
    "def process_data(example, vocab, tokenizer):\n",
    "    \"\"\"\n",
    "    Pads input and target sequences to fixed lengths and records original lengths.\n",
    "    Tokenizes the text and converts tokens to token IDs.\n",
    "\n",
    "    Args:\n",
    "        example (dict): Raw example with 'document' and 'summary'.\n",
    "        vocab (Vocab): Vocabulary object.\n",
    "        tokenizer: Tokenizer instance used for tokenizing text.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tensors for input/target IDs and their original lengths.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Tokenize raw text (both 'document' and 'summary')\n",
    "    # Place your code here\n",
    "    # input_tokens  = ... # Tokenizing the document (input)\n",
    "    # target_tokens = ... # Tokenizing the summary (target)\n",
    "\n",
    "    # Convert tokens to token IDs using the vocabulary\n",
    "    input_ids  = [vocab[token] if token in vocab else UNK_IDX for token in input_tokens]\n",
    "    target_ids = [vocab[token] if token in vocab else UNK_IDX for token in target_tokens]\n",
    "\n",
    "    input_len  = len(input_ids)   # Save length BEFORE padding\n",
    "    target_len = len(target_ids)\n",
    "\n",
    "    # TODO: Apply padding/truncation\n",
    "    # Place your code here (Hint: use defined functions)\n",
    "    # input_ids  = ...\n",
    "    # target_ids = ...\n",
    "\n",
    "    # Return the processed data as tensors, along with original lengths\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "        \"input_len\": torch.tensor(input_len, dtype=torch.long),\n",
    "        \"target_len\": torch.tensor(target_len, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply processing to the datasets\n",
    "processed_datasets = {\n",
    "    split: raw_datasets[split].map(lambda example: process_data(example, vocab, tokenizer))\n",
    "    for split in [\"train\", \"validation\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec823424",
   "metadata": {},
   "source": [
    "# Creating Dataloaders and Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Input lengths: tensor([266, 103, 129, 212, 386])\n",
      "Target lengths: tensor([23, 18, 25, 23, 24])\n"
     ]
    }
   ],
   "source": [
    "# 6. Custom Dataset Class\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch-compatible dataset wrapper for processed sequence-to-sequence data.\n",
    "    \n",
    "    This class takes tokenized, padded, and numericalized examples and allows them\n",
    "    to be used with a DataLoader to enable batching, shuffling, and parallel loading.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes the custom dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (DatasetDict): A HuggingFace-style dataset where each example is a dict\n",
    "                                   containing 'input_ids', 'target_ids', 'input_len', 'target_len'.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the sample at a specific index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing input/target sequences and their lengths.\n",
    "                  These are returned as PyTorch tensors.\n",
    "        \"\"\"\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),  # Convert to tensor\n",
    "            \"target_ids\": torch.tensor(item[\"target_ids\"], dtype=torch.long),  # Convert to tensor\n",
    "            \"input_len\": torch.tensor(item[\"input_len\"], dtype=torch.long),  # Convert to tensor\n",
    "            \"target_len\": torch.tensor(item[\"target_len\"], dtype=torch.long)  # Convert to tensor\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Instantiate PyTorch-compatible datasets from the processed HuggingFace-style splits\n",
    "train_dataset = Seq2SeqDataset(processed_datasets[\"train\"])        # For training\n",
    "valid_dataset = Seq2SeqDataset(processed_datasets[\"validation\"])   # For validation\n",
    "test_dataset  = Seq2SeqDataset(processed_datasets[\"test\"])         # For testing\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Sanity Check – Inspect One Batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Input shape:\", batch[\"input_ids\"].shape)\n",
    "print(\"Target shape:\", batch[\"target_ids\"].shape)\n",
    "print(\"Input lengths:\", batch[\"input_len\"][:5])\n",
    "print(\"Target lengths:\", batch[\"target_len\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b415dd",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "The following is a simple implementation of a LSTM-based Seq2Seq model for tasks like text summarization or machine translation.\n",
    "\n",
    "#### Questions:\n",
    "- **What is the Embedding Layer and Why is it Used?**  \n",
    "- **What is Teacher Forcing and Why is it Used?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Embedding layer to convert token IDs to embeddings\n",
    "        # Place your code here\n",
    "        # self.embedding = ...\n",
    "        # TODO: LSTM layer to process sequences and output hidden and cell states\n",
    "        # Place your code here\n",
    "        # self.lstm = ...\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Convert token IDs to embeddings\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # Process the embeddings with the LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Embedding layer to convert token IDs to embeddings\n",
    "        # Place your code here\n",
    "        # self.embedding = ...\n",
    "        # LSTM layer to process the current token and hidden state\n",
    "        # Place your code here\n",
    "        # self.lstm = ...\n",
    "        # Fully connected layer to predict the next token in the sequence\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # Convert current token to embedding\n",
    "        embedded = self.embedding(input_token.unsqueeze(1))  # Shape: (B, 1, E)\n",
    "        # Process the embedded token with the LSTM and pass hidden, cell states\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Get the logits for the next token prediction\n",
    "        logits = self.fc(output.squeeze(1))  # Shape: (B, vocab_size)\n",
    "        return logits, hidden, cell\n",
    "    \n",
    "# Seq2Seq class to combine the encoder and decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize the encoder and decoder here\n",
    "        # Place your code here\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)  # Number of sequences in the batch\n",
    "        max_len = tgt.size(1)     # Maximum length of the target sequence\n",
    "        vocab_size = self.decoder.fc.out_features  # Size of the vocabulary\n",
    "\n",
    "        # Tensor to hold all predictions (outputs) for each token\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size)\n",
    "\n",
    "        # TODO: Get initial hidden and cell states from the encoder\n",
    "        # Place your code here\n",
    "        # hidden, cell = ...\n",
    "        input_token = tgt[:, 0]  # Start token (usually <sos>)\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            # Pass the current token and states to the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output  # Store the output for the current time step\n",
    "\n",
    "            # Apply teacher forcing: decide whether to use true target or predicted token\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # Get the predicted token (max logit)\n",
    "\n",
    "            # Use the true token (from the target) if teacher forcing is applied, otherwise use predicted token\n",
    "            input_token = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05cab4",
   "metadata": {},
   "source": [
    "# Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26030fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio=0.5):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0  # Track total loss for the epoch\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move data to the device (GPU or CPU)\n",
    "        src = batch['input_ids'].to(device)\n",
    "        tgt = batch['target_ids'].to(device)\n",
    "\n",
    "        # TODO: Zero the gradients before each backpropagation\n",
    "        # Place your code here\n",
    "        \n",
    "\n",
    "        # TODO: Forward pass through the model\n",
    "        # Place your code here\n",
    "        # output = \n",
    "        \n",
    "        # Flatten the output and target for loss calculation\n",
    "        output_dim = output.shape[-1]  # Output dimension (vocab size)\n",
    "        # output = output.view(-1, output_dim)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "        # tgt = tgt[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len) (ignore <sos> token)\n",
    "        output = output[:, 1:, :].contiguous().view(-1, output_dim)  # Skip <sos> predictions\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)  # Skip <sos> targets\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, tgt)\n",
    "        epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:  # Print training progress every 10 batches\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return epoch_loss / len(train_loader)  # Return average loss for the epoch\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0  # Track total loss for the validation\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for batch in valid_loader:\n",
    "            # Move data to the device\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['target_ids'].to(device)\n",
    "\n",
    "            # TODO: Forward pass through the model\n",
    "            # Place your code here\n",
    "            # output = ... # No teacher forcing during eval\n",
    "\n",
    "\n",
    "            # Flatten the output and target for loss calculation\n",
    "            output_dim = output.shape[-1]  # Output dimension (vocab size)\n",
    "            # output = output.view(-1, output_dim)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            # tgt = tgt[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len)\n",
    "            output = output[:, 1:, :].contiguous().view(-1, output_dim)  # Skip <sos> predictions\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)  # Skip <sos> targets\n",
    "\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    return epoch_loss / len(valid_loader)  # Return average loss for the validation set\n",
    "\n",
    "\n",
    "# Training loop function\n",
    "def train_loop(model, train_loader, valid_loader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configurations\n",
    "vocab_size = len(vocab)  \n",
    "embed_dim = 128  # Dimensionality of word embeddings\n",
    "hidden_dim = 256 # Hidden state size of the LSTM\n",
    "\n",
    "# TODO: Initialize Model\n",
    "# Place your code here\n",
    "# model = \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "\n",
    "# Define number of epochs for training\n",
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "train_loop(model, train_loader, valid_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b90643",
   "metadata": {},
   "source": [
    "# Predictions vs Ground Truth (Qualitative Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, src, tgt, device, vocab):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Move the source and target to the correct device (CPU/GPU)\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    \n",
    "    # TODO: Generate output using the model (disable teacher forcing here)\n",
    "    # Place your code here\n",
    "    # output = ...  # No teacher forcing during evaluation\n",
    "\n",
    "    # Get the predicted tokens (taking argmax across vocab size)\n",
    "    predicted_tokens = output.argmax(2)  # (batch_size, seq_len)\n",
    "    \n",
    "    # TODO: Convert token IDs back to text using the vocab's get_itos() method (index-to-string)\n",
    "    # Place your code here\n",
    "    # predicted_text = ...\n",
    "    \n",
    "    # TODO: Convert the target tokens to text as well for comparison\n",
    "    # Place your code here\n",
    "    # target_text = ...\n",
    "\n",
    "    return predicted_text, target_text\n",
    "\n",
    "# Generate prediction for the first batch of test data\n",
    "src_sample = test_loader.dataset[0]['input_ids']  # First input example from the test set\n",
    "tgt_sample = test_loader.dataset[0]['target_ids']  # First target example from the test set\n",
    "\n",
    "predictions, actuals = generate_prediction(model, src_sample.unsqueeze(0), tgt_sample.unsqueeze(0), device, vocab)\n",
    "\n",
    "# Now let's print the comparison\n",
    "print(\"Predicted Text:\", predictions[0])\n",
    "print(\"Actual Target Text:\", actuals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441086d",
   "metadata": {},
   "source": [
    "## Bonus: Incorporate Attention to the Model and Evaluate the Results\n",
    "\n",
    "Incorporating **Attention** mechanisms into the Seq2Seq model can significantly improve the model's ability to focus on relevant parts of the input sequence while generating output. This is particularly useful for longer sequences where the model might struggle to capture long-range dependencies with a standard encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad44531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Place your code here\n",
    "# Hint: You can modify the main code of LSTM-based Seq2Seq model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

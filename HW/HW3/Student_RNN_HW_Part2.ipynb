{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b7b2f2b9",
      "metadata": {
        "id": "b7b2f2b9"
      },
      "source": [
        "# LSTM-based Seq2Seq Model for Abstractive Summarization\n",
        "\n",
        "You can ask your questions in Telegram : @FatemehNikkhoo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ad30cc",
      "metadata": {
        "id": "28ad30cc"
      },
      "source": [
        "Name = \"Seyyed Amirmahdi Sadrzadeh\"\n",
        "\n",
        "StudentId = \"401102015\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165b2e4e",
      "metadata": {
        "id": "165b2e4e"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86d13e81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86d13e81",
        "outputId": "70100726-33e9-44d9-d79b-6c72e5cc67aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-0b853356d24e>\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "95e4a7d1",
      "metadata": {
        "id": "95e4a7d1"
      },
      "outputs": [],
      "source": [
        "# Set up device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422ca28f",
      "metadata": {
        "id": "422ca28f"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eeb10e6",
      "metadata": {
        "id": "8eeb10e6"
      },
      "source": [
        "# Extreme Summarization (XSum) Dataset\n",
        "\n",
        "The **XSUM** dataset is designed for the task of extreme summarization, where the goal is to generate a single-sentence summary for a news article.\n",
        "\n",
        "### Features:\n",
        "- **document:** The input news article.\n",
        "- **summary:** A one-sentence summary of the article.\n",
        "- **id:** A unique BBC ID for each article.\n",
        "\n",
        "For more details and to explore the dataset, you can visit the official [Hugging Face XSUM page](https://huggingface.co/datasets/xsum).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dcb7f7a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcb7f7a3",
        "outputId": "77ada635-35e2-43a8-b427-1d71c52c6974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XSUM dataset...\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the XSUM dataset\n",
        "print(\"Loading XSUM dataset...\")\n",
        "\n",
        "# Load each split using slice syntax\n",
        "raw_datasets = {\n",
        "    \"train\": load_dataset(\"xsum\", split=\"train[:2000]\"),\n",
        "    \"validation\": load_dataset(\"xsum\", split=\"validation[:500]\"),\n",
        "    \"test\": load_dataset(\"xsum\", split=\"test[:500]\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e9ff575e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ff575e",
        "outputId": "588d8c91-fb39-4436-d414-fb4f2445a750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 500\n",
            "validation size: 100\n",
            "test size: 100\n",
            "Sample from random index: 30\n",
            "\n",
            "document: The team went into administration in October but, as revealed by BBC Sport, have secured investment from Stephen Fitzpatrick, boss of energy firm Ovo.\n",
            "Former Sainsbury's boss Justin King has joined as interim chairman.\n",
            "He said he was confident that Manor had \"the right people, the right values and sheer hard work\" and would be \"competitive at the highest level\".\n",
            "King is not financially involved in the team but will take a leading role on the business side of the operation.\n",
            "Fitzpatrick's investment is a personal one and the money he has put into the team does not come from Ovo.\n",
            "He said: \"I have a lifelong passion for F1 and can't wait for the season ahead.\"\n",
            "Manor Marussia have announced Englishman Will Stevens will be one of their drivers and said a deal to sign the second would be completed soon.\n",
            "The team's new car, a modified version of the 2014 model, must pass F1's mandatory crash tests before they can race at the season-opening Australian Grand Prix in Melbourne from 13-15 March.\n",
            "Those tests are scheduled to take place this week.\n",
            "Graeme Lowdon, who has been with the team through its various guises as Manor, Virgin and Marussia, remains as president and sporting director.\n",
            "He said: \"It has been a challenging period for all of us but we've come through it and now we just want to go racing again.\n",
            "\"With formidable new business leadership in Stephen Fitzpatrick and the board presence of Justin King, we are now in a great place ahead of the new season. This is a fantastic and very rewarding moment for all those involved with the team.\"\n",
            "\n",
            "summary: The Manor Marussia team have confirmed they intend to return to Formula 1 in time for the start of the season.\n",
            "\n",
            "id: 31723471\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Inspection\n",
        "\n",
        "# Inspect the dataset size\n",
        "for split, data in raw_datasets.items():\n",
        "    print(f\"{split} size: {len(data)}\")\n",
        "\n",
        "# Inspect a random sample of the train dataset\n",
        "train_len = len(raw_datasets['train'])\n",
        "# Select a random index between 0 and train_len - 1\n",
        "random_index = random.randint(0, train_len  - 1)\n",
        "print(f\"Sample from random index: {random_index}\\n\")\n",
        "for key in raw_datasets['train'][random_index]:\n",
        "    print(f\"{key}: {raw_datasets['train'][random_index][key]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ae830f",
      "metadata": {
        "id": "68ae830f"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "### Question:\n",
        "- What is the role of a tokenizer in Natural Language Processing (NLP)?\n",
        "- What does it mean to \"tokenize\" text, and why is this step necessary?\n",
        "\n",
        "### Tokenization in NLP\n",
        "\n",
        "\n",
        "A tokenizer is responsible for breaking down raw text into smaller units called **tokens** — which can be words, subwords, or even characters depending on the tokenizer used. This is one of the very first steps in any NLP pipeline, and it's essential for converting unstructured text into a format that a model can understand and process.\n",
        "\n",
        "\n",
        "To \"tokenize\" text means to split it into meaningful elements (tokens). For example, the sentence:\n",
        "> *\"My Deep Lerning code.\"*\n",
        "\n",
        "Could be tokenized as:\n",
        "> `[\"My\", \"Deep\", \"Learning\", \"code\", \".\"]`\n",
        "\n",
        "Or with subword tokenization:\n",
        "> `[\"My\", \"Deep\", \"lear\", \"ning\", \"code\", \".\"]`\n",
        "\n",
        "This step is necessary because **neural networks can't operate directly on raw text** — they require numerical input. Tokenization bridges this gap by turning text into tokens, which can then be mapped to numeric representations (like token IDs or embeddings). It also helps in handling vocabulary, padding, truncation, and overall consistency across samples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "263686e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "263686e5",
        "outputId": "9d86e3e8-cf72-41eb-814f-25445c30fb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized sample at index 30 from 'train' set:\n",
            "document: The team went into administration in October but, as revealed by BBC Sport, have secured investment from Stephen Fitzpatrick, boss of energy firm Ovo.\n",
            "Former Sainsbury's boss Justin King has joined as interim chairman.\n",
            "He said he was confident that Manor had \"the right people, the right values and sheer hard work\" and would be \"competitive at the highest level\".\n",
            "King is not financially involved in the team but will take a leading role on the business side of the operation.\n",
            "Fitzpatrick's investment is a personal one and the money he has put into the team does not come from Ovo.\n",
            "He said: \"I have a lifelong passion for F1 and can't wait for the season ahead.\"\n",
            "Manor Marussia have announced Englishman Will Stevens will be one of their drivers and said a deal to sign the second would be completed soon.\n",
            "The team's new car, a modified version of the 2014 model, must pass F1's mandatory crash tests before they can race at the season-opening Australian Grand Prix in Melbourne from 13-15 March.\n",
            "Those tests are scheduled to take place this week.\n",
            "Graeme Lowdon, who has been with the team through its various guises as Manor, Virgin and Marussia, remains as president and sporting director.\n",
            "He said: \"It has been a challenging period for all of us but we've come through it and now we just want to go racing again.\n",
            "\"With formidable new business leadership in Stephen Fitzpatrick and the board presence of Justin King, we are now in a great place ahead of the new season. This is a fantastic and very rewarding moment for all those involved with the team.\"\n",
            "\n",
            "summary: The Manor Marussia team have confirmed they intend to return to Formula 1 in time for the start of the season.\n",
            "\n",
            "id: 31723471\n",
            "\n",
            "input_ids: ['the', 'team', 'went', 'into', 'administration', 'in', 'october', 'but', ',', 'as', 'revealed', 'by', 'bbc', 'sport', ',', 'have', 'secured', 'investment', 'from', 'stephen', 'fitzpatrick', ',', 'boss', 'of', 'energy', 'firm', 'ovo', '.', 'former', 'sainsbury', \"'\", 's', 'boss', 'justin', 'king', 'has', 'joined', 'as', 'interim', 'chairman', '.', 'he', 'said', 'he', 'was', 'confident', 'that', 'manor', 'had', 'the', 'right', 'people', ',', 'the', 'right', 'values', 'and', 'sheer', 'hard', 'work', 'and', 'would', 'be', 'competitive', 'at', 'the', 'highest', 'level', '.', 'king', 'is', 'not', 'financially', 'involved', 'in', 'the', 'team', 'but', 'will', 'take', 'a', 'leading', 'role', 'on', 'the', 'business', 'side', 'of', 'the', 'operation', '.', 'fitzpatrick', \"'\", 's', 'investment', 'is', 'a', 'personal', 'one', 'and', 'the', 'money', 'he', 'has', 'put', 'into', 'the', 'team', 'does', 'not', 'come', 'from', 'ovo', '.', 'he', 'said', 'i', 'have', 'a', 'lifelong', 'passion', 'for', 'f1', 'and', 'can', \"'\", 't', 'wait', 'for', 'the', 'season', 'ahead', '.', 'manor', 'marussia', 'have', 'announced', 'englishman', 'will', 'stevens', 'will', 'be', 'one', 'of', 'their', 'drivers', 'and', 'said', 'a', 'deal', 'to', 'sign', 'the', 'second', 'would', 'be', 'completed', 'soon', '.', 'the', 'team', \"'\", 's', 'new', 'car', ',', 'a', 'modified', 'version', 'of', 'the', '2014', 'model', ',', 'must', 'pass', 'f1', \"'\", 's', 'mandatory', 'crash', 'tests', 'before', 'they', 'can', 'race', 'at', 'the', 'season-opening', 'australian', 'grand', 'prix', 'in', 'melbourne', 'from', '13-15', 'march', '.', 'those', 'tests', 'are', 'scheduled', 'to', 'take', 'place', 'this', 'week', '.', 'graeme', 'lowdon', ',', 'who', 'has', 'been', 'with', 'the', 'team', 'through', 'its', 'various', 'guises', 'as', 'manor', ',', 'virgin', 'and', 'marussia', ',', 'remains', 'as', 'president', 'and', 'sporting', 'director', '.', 'he', 'said', 'it', 'has', 'been', 'a', 'challenging', 'period', 'for', 'all', 'of', 'us', 'but', 'we', \"'\", 've', 'come', 'through', 'it', 'and', 'now', 'we', 'just', 'want', 'to', 'go', 'racing', 'again', '.', 'with', 'formidable', 'new', 'business', 'leadership', 'in', 'stephen', 'fitzpatrick', 'and', 'the', 'board', 'presence', 'of', 'justin', 'king', ',', 'we', 'are', 'now', 'in', 'a', 'great', 'place', 'ahead', 'of', 'the', 'new', 'season', '.', 'this', 'is', 'a', 'fantastic', 'and', 'very', 'rewarding', 'moment', 'for', 'all', 'those', 'involved', 'with', 'the', 'team', '.']\n",
            "\n",
            "target_ids: ['the', 'manor', 'marussia', 'team', 'have', 'confirmed', 'they', 'intend', 'to', 'return', 'to', 'formula', '1', 'in', 'time', 'for', 'the', 'start', 'of', 'the', 'season', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. Tokenization\n",
        "\n",
        "# Apply tokenization on the 'document' (news article) and 'summary' (highlight).\n",
        "def tokenize_function(example, tokenizer):\n",
        "    \"\"\"\n",
        "    This function takes a batch of example and applies tokenization using the provided tokenizer.\n",
        "\n",
        "    Args:\n",
        "    example (dict): A dictionary containing text data with keys like \"document\" and \"summary\".\n",
        "    tokenizer: A tokenizer instance (e.g., from `torchtext` or `transformers`).\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing tokenized inputs and target sequences with keys 'input_ids' and 'target_ids'.\n",
        "    \"\"\"\n",
        "    # TODO: Apply tokenization\n",
        "    inputs = tokenizer(example[\"document\"])  # Tokenize the article (input)\n",
        "    targets = tokenizer(example[\"summary\"])  # Tokenize the summary (target)\n",
        "    return {\"input_ids\": inputs, \"target_ids\": targets}\n",
        "\n",
        "\n",
        "# Tokenizer (using basic English tokenizer)\n",
        "tokenizer = get_tokenizer(\"basic_english\")  # Basic word-level tokenization\n",
        "\n",
        "# Applying the tokenizer function to the dataset\n",
        "tokenized_datasets = {\n",
        "    split: raw_datasets[split].map(lambda example: tokenize_function(example, tokenizer))\n",
        "    for split in [\"train\", \"validation\", \"test\"]\n",
        "}\n",
        "\n",
        "# TODO: Inspect a sample of tokenized_datasets['train'] to better understand the results\n",
        "# Print the keys and values of the sample at the random_index that was calculated earlier\n",
        "print(f\"\\nTokenized sample at index {random_index} from 'train' set:\")\n",
        "for key in tokenized_datasets[\"train\"][random_index]:\n",
        "    print(f\"{key}: {tokenized_datasets['train'][random_index][key]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f0397",
      "metadata": {
        "id": "1c4f0397"
      },
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "In NLP tasks, the vocabulary maps each token (word) to a unique integer ID.\n",
        "\n",
        "### Question:\n",
        "- What are the special characters `\"<unk>\"` and `\"<pad>\"` used for in vocabulary generation?\n",
        "- Why should we build the vocabulary using only the training data?\n",
        "\n",
        "### Special Tokens in Vocabulary\n",
        "\n",
        "\n",
        "- `\"<unk>\"` stands for **unknown token**. It's used to represent any word that is **not present in the vocabulary**. This is important when the model encounters a word it has never seen during training.\n",
        "- `\"<pad>\"` stands for **padding token**. It is used to make all sequences the same length in a batch (especially when using RNNs or transformers). Padding ensures that shorter sequences don't affect the computation during training.\n",
        "\n",
        "\n",
        "We build the vocabulary **only on the training data** to avoid **data leakage**. Including tokens from the validation or test sets could unintentionally give the model access to information it wouldn’t have during real-world inference — leading to overly optimistic performance results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d8d1a99a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8d1a99a",
        "outputId": "0d8ad11e-0b3c-4026-f98b-a6d1407f48c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 17746\n",
            "Sample tokens and their corresponding IDs:\n",
            "<unk> 0\n",
            "<pad> 1\n",
            "<sos> 2\n",
            "<eos> 3\n",
            "the 4\n",
            ". 5\n",
            ", 6\n",
            "to 7\n",
            "of 8\n",
            "a 9\n"
          ]
        }
      ],
      "source": [
        "# 3. Build Vocabulary\n",
        "def build_vocab(texts, tokenizer):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary from the provided raw text data.\n",
        "    The vocabulary maps each token (word) to a unique integer ID.\n",
        "    Special tokens like <unk> (unknown words) and <pad> (padding) are included.\n",
        "    \"\"\"\n",
        "    return build_vocab_from_iterator(map(tokenizer, texts), specials=[\"<unk>\", \"<pad>\"])\n",
        "\n",
        "# TODO: Build the vocabulary from the training data considering both 'documents' and 'summary'\n",
        "# Collect all texts (documents + summaries) from training split\n",
        "# Combine both articles and summaries from training set\n",
        "train_articles = [example[\"document\"] for example in raw_datasets[\"train\"]]\n",
        "train_summaries = [example[\"summary\"] for example in raw_datasets[\"train\"]]\n",
        "combined_texts = train_articles + train_summaries  # Use this as `texts`\n",
        "\n",
        "# Build vocabulary with special tokens, including <sos> and <eos>\n",
        "vocab = build_vocab_from_iterator(\n",
        "    map(tokenizer, combined_texts),\n",
        "    specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        ")\n",
        "\n",
        "# Set default index to <unk> for unknown words\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Inspecting the vocabulary:\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Print first 10 tokens and their IDs\n",
        "print(\"Sample tokens and their corresponding IDs:\")\n",
        "for token in list(vocab.get_itos())[:10]:\n",
        "    print(token, vocab[token])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17032849",
      "metadata": {
        "id": "17032849"
      },
      "source": [
        "## Padding Function\n",
        "\n",
        "### Question:\n",
        "- Why is padding important in data preprocessing for NLP tasks, and why should we do it?\n",
        "\n",
        "\n",
        "In NLP, input sequences (like sentences or documents) often have **different lengths**, but neural networks — especially when using batches — require inputs to have **uniform dimensions**.\n",
        "\n",
        "**Padding** solves this by adding a special `<pad>` token to shorter sequences so that all sequences in a batch have the same length. This enables efficient parallel processing on GPUs and ensures compatibility with batch-based training.\n",
        "\n",
        "Padding is especially important when:\n",
        "- Training models like RNNs or Transformers that expect fixed-length input.\n",
        "- Using masking to ignore padded values during attention or loss computation.\n",
        "\n",
        "Without padding, we'd either need to process one sequence at a time (which is inefficient) or truncate valuable information arbitrarily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c424bfe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "677a2a2ead0146329e188d4f0c347ba6",
            "aa51c92c16fb4f0cae9af9a7ef10038d",
            "4aee6df35e5e479588583e1972d0b4bf",
            "a394eb8938a449d0b927a54f49f69996",
            "53a7b523b2ee4f818a81082bfde8305c",
            "813d4fab3efd464eacb3932f53a8fa25",
            "84880710b702464badd32d633ec7beb8",
            "ef85ffd9e03246a099e0d710030cbf7f",
            "3a217c2492e44a7ca8f84e2b857e2732",
            "c0c1ece7d54a4b2398a09658d88eecc0",
            "f0fdfa829bcd48b89738647527987b78",
            "0c42af8054f3419e8ef281ebca966653",
            "1c1685401fc94e46acdd24497ff19f4a",
            "35a77d7de5c44911a8be7ae83c55ffa9",
            "d50a9c74f0214a8999c812f69ff611fd",
            "70b121f060e94488a3568c7804aed7c1",
            "4026bf7349c5445988680c7b605a95aa",
            "6ca169d30bcb47bba4e0af8d6d3a19ff",
            "3d06a6e278ea4b7787793f8b0845d2c9",
            "1cb4b3b5ab44499cac2e6ee9216ec596",
            "ae1d8786c3c84d5daf6c8d79fa0c7a56",
            "dbdfc3bad6ab4b12a3b268e2c2bbfb88",
            "a110e5fa3c004b5dac307e3c32efaa75",
            "143ce31c2da74cb89a8a54baa0abfb2b",
            "8cb4e2c423d2488da18dc18285eb0e38",
            "4bb8af8a8d814cd4b05c3f6494042a75",
            "52113421758d46e2af58287202c402af",
            "ea33be4ddf7a4b148582131581c59ed8",
            "6ab11b40407e4d74ba6c9b4164f17237",
            "861fe95ef1a24b69a63044d52f2491d8",
            "bacb608fb2174a2b8d2fb0a0a09a0e02",
            "a04d6012924f41e782b43f5426ae9083",
            "55c8041153ca4f6cbd4420350ea44723"
          ]
        },
        "id": "c424bfe0",
        "outputId": "66fd5466-0dbc-4e48-a3bc-e66bfba6e264"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "677a2a2ead0146329e188d4f0c347ba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c42af8054f3419e8ef281ebca966653"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a110e5fa3c004b5dac307e3c32efaa75"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 4. Padding function (modified to accept token IDs)\n",
        "# Constants\n",
        "MAX_LENGTH = 64  # Maximum sequence length\n",
        "PAD_IDX = vocab[\"<pad>\"]  # Padding token index\n",
        "UNK_IDX = vocab[\"<unk>\"]  # Unknown token index\n",
        "\n",
        "def pad_to_max_length(seq, max_length=MAX_LENGTH, pad_idx=PAD_IDX):\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence of token IDs to a fixed maximum length.\n",
        "    \"\"\"\n",
        "    return seq + [pad_idx] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length]\n",
        "\n",
        "# 5. Sequence processing function (ensure tokenization and conversion to token IDs)\n",
        "def process_data(example, vocab, tokenizer):\n",
        "    \"\"\"\n",
        "    Pads input and target sequences to fixed lengths and records original lengths.\n",
        "    Tokenizes the text and converts tokens to token IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize input and target\n",
        "    input_tokens = tokenizer(example[\"document\"])\n",
        "\n",
        "    # ADD <sos> and <eos> to target tokens\n",
        "    target_tokens = [\"<sos>\"] + tokenizer(example[\"summary\"]) + [\"<eos>\"]\n",
        "\n",
        "    # Convert tokens to token IDs using the vocabulary\n",
        "    input_ids  = [vocab[token] if token in vocab else UNK_IDX for token in input_tokens]\n",
        "    target_ids = [vocab[token] if token in vocab else UNK_IDX for token in target_tokens]\n",
        "\n",
        "    # Save lengths before padding\n",
        "    input_len  = len(input_ids)\n",
        "    target_len = len(target_ids)\n",
        "\n",
        "    # Pad or truncate\n",
        "    input_ids  = pad_to_max_length(input_ids)\n",
        "    target_ids = pad_to_max_length(target_ids)\n",
        "\n",
        "    # Return as plain lists (we convert to tensors later)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"target_ids\": target_ids,\n",
        "        \"input_len\": input_len,\n",
        "        \"target_len\": target_len,\n",
        "    }\n",
        "\n",
        "\n",
        "# Apply processing to the datasets\n",
        "processed_datasets = {\n",
        "    split: raw_datasets[split].map(lambda example: process_data(example, vocab, tokenizer))\n",
        "    for split in [\"train\", \"validation\", \"test\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec823424",
      "metadata": {
        "id": "ec823424"
      },
      "source": [
        "# Creating Dataloaders and Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87cb0a3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87cb0a3b",
        "outputId": "f9cf232b-8dd3-4d19-9c98-bedc9395097d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 128])\n",
            "Target shape: torch.Size([8, 128])\n",
            "Input lengths: tensor([990, 424, 501, 505, 147])\n",
            "Target lengths: tensor([20, 28, 24, 14, 16])\n"
          ]
        }
      ],
      "source": [
        "# 6. Custom Dataset Class\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch-compatible dataset wrapper for processed sequence-to-sequence data.\n",
        "\n",
        "    This class takes tokenized, padded, and numericalized examples and allows them\n",
        "    to be used with a DataLoader to enable batching, shuffling, and parallel loading.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        Initializes the custom dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (DatasetDict): A HuggingFace-style dataset where each example is a dict\n",
        "                                   containing 'input_ids', 'target_ids', 'input_len', 'target_len'.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: Total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches the sample at a specific index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing input/target sequences and their lengths.\n",
        "                  These are returned as PyTorch tensors.\n",
        "        \"\"\"\n",
        "        item = self.dataset[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),  # Convert to tensor\n",
        "            \"target_ids\": torch.tensor(item[\"target_ids\"], dtype=torch.long),  # Convert to tensor\n",
        "            \"input_len\": torch.tensor(item[\"input_len\"], dtype=torch.long),  # Convert to tensor\n",
        "            \"target_len\": torch.tensor(item[\"target_len\"], dtype=torch.long)  # Convert to tensor\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Instantiate PyTorch-compatible datasets from the processed HuggingFace-style splits\n",
        "train_dataset = Seq2SeqDataset(processed_datasets[\"train\"])        # For training\n",
        "valid_dataset = Seq2SeqDataset(processed_datasets[\"validation\"])   # For validation\n",
        "test_dataset  = Seq2SeqDataset(processed_datasets[\"test\"])         # For testing\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Sanity Check – Inspect One Batch\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Input shape:\", batch[\"input_ids\"].shape)\n",
        "print(\"Target shape:\", batch[\"target_ids\"].shape)\n",
        "print(\"Input lengths:\", batch[\"input_len\"][:5])\n",
        "print(\"Target lengths:\", batch[\"target_len\"][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b415dd",
      "metadata": {
        "id": "99b415dd"
      },
      "source": [
        "### Seq2Seq Model\n",
        "\n",
        "The following is a simple implementation of a LSTM-based Seq2Seq model for tasks like text summarization or machine translation.\n",
        "\n",
        "#### Questions:\n",
        "- **What is the Embedding Layer and Why is it Used?**  \n",
        "- **What is Teacher Forcing and Why is it Used?**  \n",
        "\n",
        "### Seq2Seq Model Concepts\n",
        "\n",
        "\n",
        "The **embedding layer** is used to map each token ID (an integer) to a dense vector of fixed size. These vectors (embeddings) capture semantic information about words in a way that’s more meaningful than one-hot encoding.\n",
        "\n",
        "For example, the words \"king\" and \"queen\" might have embeddings that are close in vector space, reflecting their semantic similarity. The embedding layer is the first step in most NLP models because it provides a way to represent words in a form that neural networks can understand and learn from.\n",
        "\n",
        "> In PyTorch, we use `nn.Embedding(vocab_size, embedding_dim)` to define it.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teacher forcing** is a training strategy used in sequence-to-sequence models, especially when generating sequences like translations or summaries.\n",
        "\n",
        "During training, at each time step, instead of feeding the model's previous prediction as input to the decoder, we feed in the **actual ground truth token** (i.e., the correct word from the target sequence). This helps the model learn faster and more accurately, especially early in training.\n",
        "\n",
        "Without teacher forcing, the model may propagate its own early mistakes through the entire sequence, leading to poor training dynamics.\n",
        "\n",
        "> Teacher forcing is typically used **only during training**, not during inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "62ffcab1",
      "metadata": {
        "id": "62ffcab1"
      },
      "outputs": [],
      "source": [
        "# Encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # TODO: Embedding layer to convert token IDs to embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # TODO: LSTM layer to process sequences and output hidden and cell states\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Convert token IDs to embeddings\n",
        "        embedded = self.embedding(input_ids)  # Shape: (B, T, E)\n",
        "        # Process the embeddings with the LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded)  # output not used here\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "# Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # TODO: Embedding layer to convert token IDs to embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # TODO: LSTM layer to process the current token and hidden state\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to predict the next token in the sequence\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        # Convert current token to embedding\n",
        "        embedded = self.embedding(input_token.unsqueeze(1))  # Shape: (B, 1, E)\n",
        "        # Process the embedded token with the LSTM and pass hidden, cell states\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: (B, 1, H)\n",
        "        # Get the logits for the next token prediction\n",
        "        logits = self.fc(output.squeeze(1))  # Shape: (B, vocab_size)\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# Seq2Seq class to combine the encoder and decoder\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        # TODO: Initialize the encoder and decoder here\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)  # Number of sequences in the batch\n",
        "        max_len = tgt.size(1)     # Maximum length of the target sequence\n",
        "        vocab_size = self.decoder.fc.out_features  # Size of the vocabulary\n",
        "\n",
        "        # Tensor to hold all predictions (outputs) for each token\n",
        "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n",
        "\n",
        "        # TODO: Get initial hidden and cell states from the encoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # First decoder input is the start-of-sequence token\n",
        "        input_token = tgt[:, 0]  # Shape: (B,)\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            # Pass the current token and states to the decoder\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[:, t] = output  # Store the output for the current time step\n",
        "\n",
        "            # Apply teacher forcing: decide whether to use true target or predicted token\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)  # Get the predicted token (max logit)\n",
        "\n",
        "            # Use the true token (from the target) if teacher forcing is applied, otherwise use predicted token\n",
        "            input_token = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de05cab4",
      "metadata": {
        "id": "de05cab4"
      },
      "source": [
        "# Training and Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26030fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26030fd9",
        "outputId": "fc397971-2797-49fd-decc-8b86e3fb7dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Batch 0/63 Loss: 9.7864\n",
            "Batch 10/63 Loss: 9.2296\n",
            "Batch 20/63 Loss: 7.9409\n",
            "Batch 30/63 Loss: 7.4734\n",
            "Batch 40/63 Loss: 7.6693\n",
            "Batch 50/63 Loss: 7.5875\n",
            "Batch 60/63 Loss: 7.3606\n",
            "Training Loss: 7.9273\n",
            "Validation Loss: 7.3422\n",
            "\n",
            "Epoch 2/5\n",
            "Batch 0/63 Loss: 6.6768\n",
            "Batch 10/63 Loss: 6.6220\n",
            "Batch 20/63 Loss: 6.6040\n",
            "Batch 30/63 Loss: 6.4740\n",
            "Batch 40/63 Loss: 6.3505\n"
          ]
        }
      ],
      "source": [
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio=0.5):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0  # Track total loss for the epoch\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move data to the device (GPU or CPU)\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['target_ids'].to(device)\n",
        "\n",
        "        # TODO: Zero the gradients before each backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: Forward pass through the model\n",
        "        output = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "        # Flatten the output and target for loss calculation\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].contiguous().view(-1, output_dim)  # Skip <sos> predictions\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)  # Skip <sos> targets\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, tgt)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return epoch_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['target_ids'].to(device)\n",
        "\n",
        "            # TODO: Forward pass through the model (no teacher forcing)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:, :].contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(valid_loader)\n",
        "\n",
        "\n",
        "# Training loop function\n",
        "def train_loop(model, train_loader, valid_loader, optimizer, criterion, num_epochs, device):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        valid_loss = evaluate(model, valid_loader, criterion, device)\n",
        "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Configurations\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 256   # Dimensionality of word embeddings\n",
        "hidden_dim = 512  # Hidden state size of the LSTM\n",
        "\n",
        "# TODO: Initialize Model\n",
        "encoder = Encoder(vocab_size, embed_dim, hidden_dim)\n",
        "decoder = Decoder(vocab_size, embed_dim, hidden_dim)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Train the model\n",
        "train_loop(model, train_loader, valid_loader, optimizer, criterion, num_epochs, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b90643",
      "metadata": {
        "id": "99b90643"
      },
      "source": [
        "# Predictions vs Ground Truth (Qualitative Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d812531d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d812531d",
        "outputId": "ea983522-9fb1-4c57-c71a-f459773d2cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text: <unk> ' s the the the the the the of the the of the in the , the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "Actual Target Text: there is a chronic need for more housing for prison <unk> in wales , according to a charity .\n"
          ]
        }
      ],
      "source": [
        "def generate_prediction(model, src, tgt, device, vocab):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Move the source and target to the correct device (CPU/GPU)\n",
        "    src = src.to(device)\n",
        "    tgt = tgt.to(device)\n",
        "\n",
        "    # TODO: Generate output using the model (disable teacher forcing here)\n",
        "    with torch.no_grad():\n",
        "        output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "\n",
        "    # Get the predicted tokens (taking argmax across vocab size)\n",
        "    predicted_tokens = output.argmax(2)  # (batch_size, seq_len)\n",
        "\n",
        "    # Get vocab index-to-token mapping\n",
        "    itos = vocab.get_itos()\n",
        "\n",
        "    # TODO: Convert token IDs back to text using the vocab's get_itos() method\n",
        "    predicted_text = []\n",
        "    for seq in predicted_tokens:\n",
        "        tokens = [itos[idx] for idx in seq if idx != PAD_IDX]\n",
        "        predicted_text.append(\" \".join(tokens))\n",
        "\n",
        "    # TODO: Convert the target tokens to text as well for comparison\n",
        "    target_text = []\n",
        "    for seq in tgt:\n",
        "        tokens = [itos[idx.item()] for idx in seq if idx.item() != PAD_IDX]\n",
        "        target_text.append(\" \".join(tokens))\n",
        "\n",
        "    return predicted_text, target_text\n",
        "\n",
        "# Generate prediction for the first batch of test data\n",
        "src_sample = test_loader.dataset[0]['input_ids']  # First input example from the test set\n",
        "tgt_sample = test_loader.dataset[0]['target_ids']  # First target example from the test set\n",
        "\n",
        "predictions, actuals = generate_prediction(model, src_sample.unsqueeze(0), tgt_sample.unsqueeze(0), device, vocab)\n",
        "\n",
        "# Now let's print the comparison\n",
        "print(\"Predicted Text:\", predictions[0])\n",
        "print(\"Actual Target Text:\", actuals[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d441086d",
      "metadata": {
        "id": "d441086d"
      },
      "source": [
        "## Bonus: Incorporate Attention to the Model and Evaluate the Results\n",
        "\n",
        "Incorporating **Attention** mechanisms into the Seq2Seq model can significantly improve the model's ability to focus on relevant parts of the input sequence while generating output. This is particularly useful for longer sequences where the model might struggle to capture long-range dependencies with a standard encoder-decoder architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad44531",
      "metadata": {
        "id": "2ad44531"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# Place your code here\n",
        "# Hint: You can modify the main code of LSTM-based Seq2Seq model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "phd_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "677a2a2ead0146329e188d4f0c347ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa51c92c16fb4f0cae9af9a7ef10038d",
              "IPY_MODEL_4aee6df35e5e479588583e1972d0b4bf",
              "IPY_MODEL_a394eb8938a449d0b927a54f49f69996"
            ],
            "layout": "IPY_MODEL_53a7b523b2ee4f818a81082bfde8305c"
          }
        },
        "aa51c92c16fb4f0cae9af9a7ef10038d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_813d4fab3efd464eacb3932f53a8fa25",
            "placeholder": "​",
            "style": "IPY_MODEL_84880710b702464badd32d633ec7beb8",
            "value": "Map: 100%"
          }
        },
        "4aee6df35e5e479588583e1972d0b4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef85ffd9e03246a099e0d710030cbf7f",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a217c2492e44a7ca8f84e2b857e2732",
            "value": 2000
          }
        },
        "a394eb8938a449d0b927a54f49f69996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c1ece7d54a4b2398a09658d88eecc0",
            "placeholder": "​",
            "style": "IPY_MODEL_f0fdfa829bcd48b89738647527987b78",
            "value": " 2000/2000 [00:02&lt;00:00, 815.65 examples/s]"
          }
        },
        "53a7b523b2ee4f818a81082bfde8305c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813d4fab3efd464eacb3932f53a8fa25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84880710b702464badd32d633ec7beb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef85ffd9e03246a099e0d710030cbf7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a217c2492e44a7ca8f84e2b857e2732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0c1ece7d54a4b2398a09658d88eecc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0fdfa829bcd48b89738647527987b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c42af8054f3419e8ef281ebca966653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c1685401fc94e46acdd24497ff19f4a",
              "IPY_MODEL_35a77d7de5c44911a8be7ae83c55ffa9",
              "IPY_MODEL_d50a9c74f0214a8999c812f69ff611fd"
            ],
            "layout": "IPY_MODEL_70b121f060e94488a3568c7804aed7c1"
          }
        },
        "1c1685401fc94e46acdd24497ff19f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4026bf7349c5445988680c7b605a95aa",
            "placeholder": "​",
            "style": "IPY_MODEL_6ca169d30bcb47bba4e0af8d6d3a19ff",
            "value": "Map: 100%"
          }
        },
        "35a77d7de5c44911a8be7ae83c55ffa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d06a6e278ea4b7787793f8b0845d2c9",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cb4b3b5ab44499cac2e6ee9216ec596",
            "value": 500
          }
        },
        "d50a9c74f0214a8999c812f69ff611fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1d8786c3c84d5daf6c8d79fa0c7a56",
            "placeholder": "​",
            "style": "IPY_MODEL_dbdfc3bad6ab4b12a3b268e2c2bbfb88",
            "value": " 500/500 [00:00&lt;00:00, 808.99 examples/s]"
          }
        },
        "70b121f060e94488a3568c7804aed7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4026bf7349c5445988680c7b605a95aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ca169d30bcb47bba4e0af8d6d3a19ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d06a6e278ea4b7787793f8b0845d2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb4b3b5ab44499cac2e6ee9216ec596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae1d8786c3c84d5daf6c8d79fa0c7a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbdfc3bad6ab4b12a3b268e2c2bbfb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a110e5fa3c004b5dac307e3c32efaa75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_143ce31c2da74cb89a8a54baa0abfb2b",
              "IPY_MODEL_8cb4e2c423d2488da18dc18285eb0e38",
              "IPY_MODEL_4bb8af8a8d814cd4b05c3f6494042a75"
            ],
            "layout": "IPY_MODEL_52113421758d46e2af58287202c402af"
          }
        },
        "143ce31c2da74cb89a8a54baa0abfb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea33be4ddf7a4b148582131581c59ed8",
            "placeholder": "​",
            "style": "IPY_MODEL_6ab11b40407e4d74ba6c9b4164f17237",
            "value": "Map: 100%"
          }
        },
        "8cb4e2c423d2488da18dc18285eb0e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861fe95ef1a24b69a63044d52f2491d8",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bacb608fb2174a2b8d2fb0a0a09a0e02",
            "value": 500
          }
        },
        "4bb8af8a8d814cd4b05c3f6494042a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04d6012924f41e782b43f5426ae9083",
            "placeholder": "​",
            "style": "IPY_MODEL_55c8041153ca4f6cbd4420350ea44723",
            "value": " 500/500 [00:00&lt;00:00, 775.75 examples/s]"
          }
        },
        "52113421758d46e2af58287202c402af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea33be4ddf7a4b148582131581c59ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab11b40407e4d74ba6c9b4164f17237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "861fe95ef1a24b69a63044d52f2491d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bacb608fb2174a2b8d2fb0a0a09a0e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a04d6012924f41e782b43f5426ae9083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55c8041153ca4f6cbd4420350ea44723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
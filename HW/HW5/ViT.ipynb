{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full Name: **[Full Name]**\n",
    "- Student ID: **[Stundet ID]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Homework 4: Vision Transformer (ViT)\n",
    "\n",
    "## üìå Objective\n",
    "In this assignment, you will **implement a Vision Transformer (ViT) from scratch** and gain a deep understanding of its core components. You will:\n",
    "\n",
    "- üõ†Ô∏è Complete all core blocks of the Vision Transformer model.\n",
    "- üß™ Train the ViT on the CIFAR-10 image classification dataset.\n",
    "- üëÅÔ∏è Visualize attention maps to interpret model behavior.\n",
    "- üìä Analyze the effect of hyperparameters (e.g., patch size, depth, heads) on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Goals\n",
    "By the end of this assignment, you should be able to:\n",
    "- Explain the role of patch embeddings, self-attention, and MLP layers in ViT.\n",
    "- Implement Transformer blocks without relying on high-level libraries like Hugging Face Transformers.\n",
    "- Visualize self-attention maps to understand how ViT focuses on different parts of an image.\n",
    "- Experiment with architectural design choices and evaluate their effects.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Dataset\n",
    "We will use the **CIFAR-10** dataset, which consists of 60,000 32√ó32 color images in 10 classes, with 6,000 images per class. The dataset is split into 50,000 training and 10,000 test images.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Evaluation Criteria\n",
    "\n",
    "Your final submission will be evaluated based on:\n",
    "- ‚úÖ Correct implementation of all model components.\n",
    "- ‚úÖ Accuracy of the model on CIFAR-10 test set.\n",
    "- ‚úÖ Insightfulness of attention visualizations.\n",
    "- ‚úÖ Clarity of code and documentation.\n",
    "- ‚úÖ Quality of hyperparameter analysis and discussion.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1.1 Setup & Imports\n",
    "# ===============================\n",
    "!pip install einops torch torchvision matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1.2 Data Loading & Visualization\n",
    "# ===============================\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
    "])\n",
    "\n",
    "## To Do: Load CIFAR10:\n",
    "train_ds = ...\n",
    "test_ds = ...\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TO do: Visualize 8 images with their labels\n",
    "classes = train_ds.classes\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(1, 8, figsize=(15, 2))\n",
    "for i in range(8):\n",
    "    ...\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1.3 Helper: pair\n",
    "# ===============================\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1.4 FeedForward\n",
    "# ===============================\n",
    "## To Do: Complete the FeedForward module -> layer norm -> Linear -> GELU -> Dropout -> Linear -> Dropout\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1.5 Attention\n",
    "# ===============================\n",
    "## To Do: Complete the Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        ## layer norm\n",
    "        self.norm = ...\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        ## softmax\n",
    "        self.attend = ...\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
    "\n",
    "        ## q, k multiplication \n",
    "        dots = ...\n",
    "        ## softmax\n",
    "        attn = ...\n",
    "        ### dropout\n",
    "        attn = ...\n",
    "        ## v multiplication \n",
    "        out = ...\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1.6 Transformer Block\n",
    "# ===============================\n",
    "## To Do: Complete the Transformer module\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ## Attention + FeedForward\n",
    "            nn.ModuleList([\n",
    "                ..., \n",
    "                ...\n",
    "            ]) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            ## To do: Normal Transformer forward, nothing weird\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1.7 ViT Model\n",
    "# ===============================\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes,\n",
    "                 dim, depth, heads, mlp_dim,\n",
    "                 pool='cls', channels=3,\n",
    "                 dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "                      p1=patch_height, p2=patch_width),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    ## To do: complete the forward function\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += ...\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = ...\n",
    "\n",
    "        x = ... if self.pool == 'cls' else ...\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1.8 Training Loop & Visualization\n",
    "# ===============================\n",
    "## To do: Complete the training loop\n",
    "def train(model, epochs=10, lr=3e-4):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            ## Forward, loss and backward here\n",
    "            \n",
    "            \n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Run and enjoy!\n",
    "# ===============================\n",
    "model = ViT(\n",
    "    image_size=32, patch_size=4, num_classes=10,\n",
    "    dim=128, depth=6, heads=8, mlp_dim=256,\n",
    "    pool='cls', channels=3, dim_head=64,\n",
    "    dropout=0.1, emb_dropout=0.1\n",
    ")\n",
    "\n",
    "train(model, epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Bonus (Optional)\n",
    "- üß© Implement a variant of ViT using sinusoidal positional embeddings.\n",
    "- üé® Visualize per-head attention maps across different layers.\n",
    "- üîÅ Try training with different patch sizes and analyze effects.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full Name: **[Full Name]**\n",
    "- Student ID: **[Stundet ID]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Homework 4: Transformer for Sentiment Analysis\n",
    "\n",
    "## üìå Objective\n",
    "\n",
    "In this assignment, you will **implement a Transformer-based model for sentiment analysis** on the IMDb movie review dataset. You will:\n",
    "\n",
    "- üßπ Preprocess and clean real-world text data.\n",
    "- üèóÔ∏è Build a Transformer classifier from scratch (including positional encoding).\n",
    "- üß† Train the model to classify IMDb reviews as **positive** or **negative**.\n",
    "- üìà Evaluate model performance on the test set.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Goals\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "\n",
    "- Understand how the Transformer encoder works in NLP.\n",
    "- Implement tokenization, padding, and vocabulary creation.\n",
    "- Train a Transformer-based model for text classification.\n",
    "- Measure and interpret model performance on a real-world dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Dataset\n",
    "\n",
    "We use the **IMDb movie reviews dataset**:\n",
    "\n",
    "- Contains 50,000 highly polar movie reviews (25,000 for training and 25,000 for testing).\n",
    "- Each review is labeled as either **positive (1)** or **negative (0)**.\n",
    "- You will clean the raw text, tokenize it, and build a vocabulary before training.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Model Architecture\n",
    "\n",
    "You will build a **Transformer Encoder** model that includes:\n",
    "\n",
    "- Word Embedding Layer\n",
    "- Positional Encoding Layer\n",
    "- Multi-head Self-Attention Blocks\n",
    "- Feedforward Layers\n",
    "- Final Classification Head\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Training Details\n",
    "\n",
    "- Optimizer: `Adam`\n",
    "- Loss Function: `CrossEntropyLoss`\n",
    "- Batch Size: `32`\n",
    "- Learning Rate: `1e-3`\n",
    "- Epochs: `5`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Evaluation Criteria\n",
    "\n",
    "Your final implementation will be evaluated on:\n",
    "\n",
    "- ‚úÖ Correct implementation of the Transformer classifier.\n",
    "- ‚úÖ Clean and modular code (e.g., `Dataset`, `Dataloader`, `Model`, `Train` functions).\n",
    "- ‚úÖ Accuracy on the IMDb test set.\n",
    "- ‚úÖ Proper text preprocessing and vocabulary handling.\n",
    "- ‚úÖ Well-commented and readable code.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit part\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def download_imdb(data_path=\"./imdb\"):\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    filepath = os.path.join(data_path, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Downloading IMDb dataset...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024): f.write(chunk)\n",
    "\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
    "            tar.extractall(path=data_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "download_imdb()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "def load_imdb_data(base_path, split='train'):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(base_path, f'aclImdb/{split}/{label}')\n",
    "        for fname in os.listdir(folder):\n",
    "            with open(os.path.join(folder, fname), 'r', encoding='utf8') as f:\n",
    "                text = clean_text(f.read())\n",
    "                data.append((text, 1 if label == 'pos' else 0))\n",
    "    return data\n",
    "\n",
    "train_raw = load_imdb_data(\"./imdb\", split='train')\n",
    "test_raw = load_imdb_data(\"./imdb\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit part\n",
    "def tokenize(text): \n",
    "    return text.split()\n",
    "\n",
    "# Build vocab\n",
    "counter = Counter()\n",
    "for text, _ in train_raw:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for word, freq in counter.items():\n",
    "    if freq >= 5:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab.get(w, vocab[\"<unk>\"]) for w in tokenize(text)]\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [(encode(text), label) for text, label in data]\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = [torch.tensor(x) for x in texts]\n",
    "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return texts.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "train_dataset = IMDBDataset(train_raw)\n",
    "test_dataset = IMDBDataset(test_raw)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: complete the PositionalEncoding module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        ## Sin , Cos positional encoding\n",
    "        pe[:, 0::2] = ...\n",
    "        pe[:, 1::2] = ...\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: complete MultiheadAttention module\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.q_proj = ...\n",
    "        self.k_proj = ...\n",
    "        self.v_proj = ...\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        batch_size = query.size(1)\n",
    "        \n",
    "        # Project inputs to query, key, value\n",
    "        q = self.q_proj(query).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        k = self.k_proj(key).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        v = self.v_proj(value).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_output, attn_weights = ...\n",
    "        \n",
    "        # Concatenate heads and project back to original dimension\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(-1, batch_size, self.embed_dim)\n",
    "        attn_output = ...\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    ## To do\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        attn_scores = ....\n",
    "        attn_weights = ...\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: complete the forward function\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self attention\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = ...\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Feedforward\n",
    "        src2 = ...\n",
    "        src = ...\n",
    "        src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        \n",
    "    ## To do\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = ...\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, d_model*2, dropout=0.1)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        ## get the embedding\n",
    "        x = ...\n",
    "        ## pos encode\n",
    "        x = ...\n",
    "        x = ...  # (seq_len, batch, dim)\n",
    "        return self.fc(x[0])  # Use first token as representation\n",
    "model = TransformerClassifier(len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: complete the training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        ...\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and enjoy\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Train Acc {train_acc:.4f}, Test Acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Bonus (Optional)\n",
    "\n",
    "- üîç Experiment with different model hyperparameters (e.g., `nhead`, `d_model`, `num_layers`).\n",
    "- üìä Plot training and validation accuracy over epochs.\n",
    "- üìå Use attention weights to interpret model focus.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

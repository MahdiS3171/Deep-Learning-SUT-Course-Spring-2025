{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-EnY-Vm1a4V"
      },
      "source": [
        "- Full Name: **Seyyed Amirmahdi Sadrzadeh**\n",
        "- Student ID: **401102015**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbdExQRd1a4X"
      },
      "source": [
        "# üß† Homework 4: Transformer for Sentiment Analysis\n",
        "\n",
        "## üìå Objective\n",
        "\n",
        "In this assignment, you will **implement a Transformer-based model for sentiment analysis** on the IMDb movie review dataset. You will:\n",
        "\n",
        "- üßπ Preprocess and clean real-world text data.\n",
        "- üèóÔ∏è Build a Transformer classifier from scratch (including positional encoding).\n",
        "- üß† Train the model to classify IMDb reviews as **positive** or **negative**.\n",
        "- üìà Evaluate model performance on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learning Goals\n",
        "\n",
        "By the end of this assignment, you should be able to:\n",
        "\n",
        "- Understand how the Transformer encoder works in NLP.\n",
        "- Implement tokenization, padding, and vocabulary creation.\n",
        "- Train a Transformer-based model for text classification.\n",
        "- Measure and interpret model performance on a real-world dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Dataset\n",
        "\n",
        "We use the **IMDb movie reviews dataset**:\n",
        "\n",
        "- Contains 50,000 highly polar movie reviews (25,000 for training and 25,000 for testing).\n",
        "- Each review is labeled as either **positive (1)** or **negative (0)**.\n",
        "- You will clean the raw text, tokenize it, and build a vocabulary before training.\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Model Architecture\n",
        "\n",
        "You will build a **Transformer Encoder** model that includes:\n",
        "\n",
        "- Word Embedding Layer\n",
        "- Positional Encoding Layer\n",
        "- Multi-head Self-Attention Blocks\n",
        "- Feedforward Layers\n",
        "- Final Classification Head\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Training Details\n",
        "\n",
        "- Optimizer: `Adam`\n",
        "- Loss Function: `CrossEntropyLoss`\n",
        "- Batch Size: `32`\n",
        "- Learning Rate: `1e-3`\n",
        "- Epochs: `5`\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Evaluation Criteria\n",
        "\n",
        "Your final implementation will be evaluated on:\n",
        "\n",
        "- ‚úÖ Correct implementation of the Transformer classifier.\n",
        "- ‚úÖ Clean and modular code (e.g., `Dataset`, `Dataloader`, `Model`, `Train` functions).\n",
        "- ‚úÖ Accuracy on the IMDb test set.\n",
        "- ‚úÖ Proper text preprocessing and vocabulary handling.\n",
        "- ‚úÖ Well-commented and readable code.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S8nIa0oa1a4Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxuxXC01a4Z",
        "outputId": "3091c081-3a6b-46c3-b094-799318353758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading IMDb dataset...\n",
            "Extracting...\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "## Do not edit part\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def download_imdb(data_path=\"./imdb\"):\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    filepath = os.path.join(data_path, \"aclImdb_v1.tar.gz\")\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(\"Downloading IMDb dataset...\")\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024): f.write(chunk)\n",
        "\n",
        "        print(\"Extracting...\")\n",
        "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_path)\n",
        "    print(\"Done.\")\n",
        "\n",
        "download_imdb()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text.lower()\n",
        "\n",
        "def load_imdb_data(base_path, split='train'):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = os.path.join(base_path, f'aclImdb/{split}/{label}')\n",
        "        for fname in os.listdir(folder):\n",
        "            with open(os.path.join(folder, fname), 'r', encoding='utf8') as f:\n",
        "                text = clean_text(f.read())\n",
        "                data.append((text, 1 if label == 'pos' else 0))\n",
        "    return data\n",
        "\n",
        "train_raw = load_imdb_data(\"./imdb\", split='train')\n",
        "test_raw = load_imdb_data(\"./imdb\", split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m4HWhtXa1a4Z"
      },
      "outputs": [],
      "source": [
        "## Do not edit part\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Build vocab\n",
        "counter = Counter()\n",
        "for text, _ in train_raw:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "for word, freq in counter.items():\n",
        "    if freq >= 5:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "def encode(text):\n",
        "    return [vocab.get(w, vocab[\"<unk>\"]) for w in tokenize(text)]\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = [(encode(text), label) for text, label in data]\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = [torch.tensor(x) for x in texts]\n",
        "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return texts.to(device), torch.tensor(labels).to(device)\n",
        "\n",
        "train_dataset = IMDBDataset(train_raw)\n",
        "test_dataset = IMDBDataset(test_raw)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6gVrcM9v1a4Z"
      },
      "outputs": [],
      "source": [
        "## To do: complete the PositionalEncoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "        ## Sin , Cos positional encoding\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)        # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)        # odd dims\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Km7lMwdL1a4a"
      },
      "outputs": [],
      "source": [
        "## To do: complete MultiheadAttention module\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None):\n",
        "        batch_size = query.size(1)\n",
        "\n",
        "        # Project inputs to query, key, value\n",
        "        q = self.q_proj(query).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        k = self.k_proj(key).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        v = self.v_proj(value).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        # Concatenate heads and project back to original dimension\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(-1, batch_size, self.embed_dim)\n",
        "        attn_output = self.dropout(self.out_proj(attn_output))\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    ## To do\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        attn_scores = torch.bmm(q, k.transpose(1, 2)) / np.sqrt(self.head_dim)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dSdJkd3C1a4a"
      },
      "outputs": [],
      "source": [
        "## To do: complete the forward function\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Self attention\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feedforward\n",
        "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LFGEz9j71a4a"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "\n",
        "    ## To do\n",
        "    def forward(self, src):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z3ojY_Tr1a4a"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, d_model*2, dropout=0.1)\n",
        "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, src):\n",
        "        ## get the embedding\n",
        "        x = self.embedding(src)\n",
        "        ## pos encode\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.permute(1, 0, 2)   # (seq_len, batch, dim)\n",
        "        x = self.transformer(x)\n",
        "        return self.fc(x[0])  # Use first token as representation\n",
        "model = TransformerClassifier(len(vocab)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YdusbqTx1a4b"
      },
      "outputs": [],
      "source": [
        "## To do: complete the training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for x, y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct    += (out.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            out = model(x)\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgXzcNOB1a4b",
        "outputId": "73518932-a94c-4a24-9af3-cfebe8f647a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss 0.5923, Train Acc 0.6885, Test Acc 0.6845\n",
            "Epoch 2: Train Loss 0.5996, Train Acc 0.6794, Test Acc 0.6487\n",
            "Epoch 3: Train Loss 0.6078, Train Acc 0.6720, Test Acc 0.6748\n",
            "Epoch 4: Train Loss 0.5844, Train Acc 0.6966, Test Acc 0.6848\n",
            "Epoch 5: Train Loss 0.5936, Train Acc 0.6892, Test Acc 0.6618\n",
            "Epoch 6: Train Loss 0.5986, Train Acc 0.6721, Test Acc 0.6918\n",
            "Epoch 7: Train Loss 0.5494, Train Acc 0.7220, Test Acc 0.7378\n",
            "Epoch 8: Train Loss 0.5393, Train Acc 0.7330, Test Acc 0.7312\n",
            "Epoch 9: Train Loss 0.5330, Train Acc 0.7370, Test Acc 0.7374\n",
            "Epoch 10: Train Loss 0.5252, Train Acc 0.7412, Test Acc 0.7369\n",
            "Epoch 11: Train Loss 0.5198, Train Acc 0.7480, Test Acc 0.7436\n",
            "Epoch 12: Train Loss 0.5054, Train Acc 0.7586, Test Acc 0.7472\n",
            "Epoch 13: Train Loss 0.5006, Train Acc 0.7628, Test Acc 0.7480\n",
            "Epoch 14: Train Loss 0.4905, Train Acc 0.7668, Test Acc 0.7528\n",
            "Epoch 15: Train Loss 0.4743, Train Acc 0.7781, Test Acc 0.7657\n",
            "Epoch 16: Train Loss 0.4748, Train Acc 0.7771, Test Acc 0.7671\n",
            "Epoch 17: Train Loss 0.4683, Train Acc 0.7837, Test Acc 0.7682\n",
            "Epoch 18: Train Loss 0.4689, Train Acc 0.7848, Test Acc 0.7612\n",
            "Epoch 19: Train Loss 0.4634, Train Acc 0.7879, Test Acc 0.7683\n",
            "Epoch 20: Train Loss 0.4501, Train Acc 0.7958, Test Acc 0.7581\n"
          ]
        }
      ],
      "source": [
        "## Run and enjoy\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss, train_acc = train(model, train_loader)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Train Acc {train_acc:.4f}, Test Acc {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06Arh721a4b"
      },
      "source": [
        "## üìù Bonus (Optional)\n",
        "\n",
        "- üîç Experiment with different model hyperparameters (e.g., `nhead`, `d_model`, `num_layers`).\n",
        "- üìä Plot training and validation accuracy over epochs.\n",
        "- üìå Use attention weights to interpret model focus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Download IMDb dataset if not exists\n",
        "def download_imdb(data_path=\"./imdb\"):\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    filepath = os.path.join(data_path, \"aclImdb_v1.tar.gz\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(\"Downloading IMDb dataset...\")\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024): f.write(chunk)\n",
        "        print(\"Extracting...\")\n",
        "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_path)\n",
        "    print(\"Done.\")\n",
        "\n",
        "download_imdb()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text.lower()\n",
        "\n",
        "def load_imdb_data(base_path, split='train'):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = os.path.join(base_path, f'aclImdb/{split}/{label}')\n",
        "        for fname in os.listdir(folder):\n",
        "            with open(os.path.join(folder, fname), 'r', encoding='utf8') as f:\n",
        "                text = clean_text(f.read())\n",
        "                data.append((text, 1 if label == 'pos' else 0))\n",
        "    return data\n",
        "\n",
        "train_raw = load_imdb_data(\"./imdb\", split='train')\n",
        "test_raw = load_imdb_data(\"./imdb\", split='test')\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Build vocab\n",
        "counter = Counter()\n",
        "for text, _ in train_raw:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "for word, freq in counter.items():\n",
        "    if freq >= 5:\n",
        "        vocab[word] = len(vocab)\n",
        "vocab_inv = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "def encode(text):\n",
        "    return [vocab.get(w, vocab[\"<unk>\"]) for w in tokenize(text)]\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = [(encode(text), label) for text, label in data]\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = [torch.tensor(x) for x in texts]\n",
        "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return texts.to(device), torch.tensor(labels).to(device)\n",
        "\n",
        "train_dataset = IMDBDataset(train_raw)\n",
        "test_dataset = IMDBDataset(test_raw)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, return_weights=False):\n",
        "        batch_size = query.size(1)\n",
        "\n",
        "        q = self.q_proj(query).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        k = self.k_proj(key).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        v = self.v_proj(value).view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v)\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(-1, batch_size, self.embed_dim)\n",
        "        attn_output = self.dropout(self.out_proj(attn_output))\n",
        "\n",
        "        if return_weights:\n",
        "            return attn_output, attn_weights\n",
        "        return attn_output, None\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        scores = torch.bmm(q, k.transpose(1, 2)) / np.sqrt(self.head_dim)\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "        output = torch.bmm(weights, v)\n",
        "        return output, weights\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        ff = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(ff)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "    def forward(self, src):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        return output\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, d_model*2, dropout=0.1)\n",
        "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "    def forward(self, src):\n",
        "        x = self.embedding(src)              # (batch, seq_len, d_model)\n",
        "        x = self.pos_encoder(x)              # add positional encoding\n",
        "        x = x.permute(1, 0, 2)               # (seq_len, batch, d_model)\n",
        "        x = self.transformer(x)              # encode\n",
        "        return self.fc(x[0])                 # use first token for classification\n",
        "\n",
        "# Initialize model, criterion, optimizer\n",
        "model = TransformerClassifier(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss, correct = 0.0, 0\n",
        "    for x, y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            out = model(x)\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Train for some epochs and track accuracy\n",
        "num_epochs = 5\n",
        "train_acc_history = []\n",
        "test_acc_history = []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss, train_acc = train(model, train_loader)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "\n",
        "    train_acc_history.append(train_acc)\n",
        "    test_acc_history.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}\")\n",
        "\n",
        "# Plot training & test accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs+1), train_acc_history, label=\"Train Acc\")\n",
        "plt.plot(range(1, num_epochs+1), test_acc_history, label=\"Test Acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train/Test Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# BONUS: Visualize attention weights from first encoder layer on a sample input\n",
        "sample_idx = 0\n",
        "sample_text, sample_label = test_dataset[sample_idx]\n",
        "sample_tensor = sample_text.unsqueeze(0).to(device)  # batch=1\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = model.embedding(sample_tensor)\n",
        "    x = model.pos_encoder(x)\n",
        "    x = x.permute(1,0,2)  # (seq_len, batch, d_model)\n",
        "\n",
        "    # extract first layer self-attention weights\n",
        "    attn_module = model.transformer.layers[0].self_attn\n",
        "    # get attn weights from forward\n",
        "    _, attn_weights = attn_module(x, x, x, return_weights=True)\n",
        "\n",
        "# attn_weights shape: (heads * batch, seq_len, seq_len)\n",
        "num_heads = attn_module.num_heads\n",
        "attn_weights = attn_weights.view(num_heads, 1, attn_weights.size(-2), attn_weights.size(-1)).squeeze(1)\n",
        "\n",
        "tokens = [vocab_inv[idx.item()] for idx in sample_text]\n",
        "\n",
        "fig, axs = plt.subplots(1, num_heads, figsize=(num_heads * 3, 3))\n",
        "for i in range(num_heads):\n",
        "    ax = axs[i] if num_heads > 1 else axs\n",
        "    im = ax.imshow(attn_weights[i].cpu(), aspect='auto')\n",
        "    ax.set_xticks(range(len(tokens)))\n",
        "    ax.set_xticklabels(tokens, rotation=90, fontsize=6)\n",
        "    ax.set_yticks(range(len(tokens)))\n",
        "    ax.set_yticklabels(tokens, fontsize=6)\n",
        "    ax.set_title(f\"Head {i}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "PbEtgBOf8sGV",
        "outputId": "548ed16d-6f82-4f14-aa0f-61520992f687"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.56 GiB is free. Process 2557 has 13.18 GiB memory in use. Of the allocated memory 10.19 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# add positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (seq_len, batch, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# use first token for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, return_weights)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-398d70d58e6e>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.56 GiB is free. Process 2557 has 13.18 GiB memory in use. Of the allocated memory 10.19 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}